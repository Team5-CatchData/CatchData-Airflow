import random
from datetime import datetime, timedelta, timezone

import pandas as pd
import requests
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.common.sql.operators.sql import SQLExecuteQueryOperator
from airflow.providers.postgres.hooks.postgres import PostgresHook
from airflow.operators.trigger_dagrun import TriggerDagRunOperator

# =========================
# ê¸°ë³¸ ì„¤ì •
# =========================
REDSHIFT_CONN_ID = "redshift_conn"
SCHEMA_NAME = "analytics"

BASE_TABLE = f"{SCHEMA_NAME}.derived_features_base"
REALTIME_TABLE = f"{SCHEMA_NAME}.realtime_waiting"
FINAL_TABLE_NAME = "realtime_waiting"
SLACK_WEBHOOK_URL = ("https://hooks.slack.com/services/T09SZ0BSHEU"
                     "/B0A3W3R4H9D/Ea5DqrFBnQKc3SzbSuNhcmZo")

# â”€â”€ ì¶”ì²œ ì „ëµ ê°€ì¤‘ì¹˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
QUALITY_W1, QUALITY_W2, QUALITY_W3 = 0.6, 0.2, 0.2
BALANCED_W1, BALANCED_W2, BALANCED_W3 = 0.4, 0.35, 0.25
CONVENIENCE_W1, CONVENIENCE_W2, CONVENIENCE_W3 = 0.3, 0.2, 0.5


# =========================
# ì‹¤ì‹œê°„ í…Œì´ë¸” ìƒì„± SQL
# =========================
REALTIME_TABLE_CREATE_SQL = f"""
CREATE TABLE IF NOT EXISTS {REALTIME_TABLE} (
    id BIGINT,
    current_visitors NUMERIC(18, 4),
    waiting INTEGER,
    rec_quality NUMERIC(18, 6),
    rec_balanced NUMERIC(18, 6),
    rec_convenience NUMERIC(18, 6),
    calculation_timestamp TIMESTAMP
)
DISTKEY(id)
SORTKEY(calculation_timestamp);
"""


# =========================
# ì‹¤ì‹œê°„ ê³„ì‚° íŒŒì´í”„ë¼ì¸
# =========================
def calculate_realtime_scores():
    KST = timezone(timedelta(hours=9))
    now = pd.Timestamp(datetime.now(KST))
    now_hour = now.hour
    TIME_COLUMN = f"time{now_hour}"
    redshift_hook = PostgresHook(postgres_conn_id=REDSHIFT_CONN_ID)
    engine = redshift_hook.get_sqlalchemy_engine()

    # 1. ì •ì  í”¼ì²˜ ë¡œë“œ
    sql = f"""
        SELECT
            id,
            base_population,
            quality_score,
            rating,
            {TIME_COLUMN}
        FROM {BASE_TABLE};
    """
    df = redshift_hook.get_pandas_df(sql)

    if df.empty:
        print("âš ï¸ base í…Œì´ë¸” ë¹„ì–´ ìˆìŒ â†’ ì¢…ë£Œ")
        return

    # 2. ì‹œê°„ëŒ€ ë¹„ìœ¨ ê³„ì‚°
    max_visits = df[TIME_COLUMN].max()
    min_visits = df[TIME_COLUMN].min()
    divisor = max(max_visits - min_visits, 1)

    def calc(row):
        hour_ratio = (row[TIME_COLUMN] - min_visits) / divisor

        current_visitors = row["base_population"] * hour_ratio

        # ë°©ë¬¸ì ê±°ì˜ ì—†ìœ¼ë©´ ëŒ€ê¸° ì—†ìŒ
        if current_visitors < 1:
            return pd.Series([current_visitors, 0])

        expected_waiting = current_visitors * (0.15 + hour_ratio * 0.25)
        variation = expected_waiting * 0.3

        waiting = int(random.normalvariate(expected_waiting, variation))
        waiting = max(0, min(waiting, int(current_visitors)))

        return pd.Series([current_visitors, waiting])

    df[["current_visitors", "waiting"]] = df.apply(calc, axis=1)

    # 3. ì •ê·œí™”
    df["quality_norm"] = df["quality_score"] / (df["quality_score"] + 10)
    df["visitors_norm"] = df["current_visitors"] / (df["current_visitors"] + 20)
    df["waiting_norm"] = df["waiting"] / (df["waiting"] + 10)

    # 4. ì¶”ì²œ ì ìˆ˜ (3ê°€ì§€ ì „ëµ)
    df["rec_quality"] = (
        QUALITY_W1 * df["quality_norm"]
        + QUALITY_W2 * df["visitors_norm"]
        - QUALITY_W3 * df["waiting_norm"]
    )

    df["rec_balanced"] = (
        BALANCED_W1 * df["quality_norm"]
        + BALANCED_W2 * df["visitors_norm"]
        - BALANCED_W3 * df["waiting_norm"]
    )

    df["rec_convenience"] = (
        CONVENIENCE_W1 * df["quality_norm"]
        + CONVENIENCE_W2 * df["visitors_norm"]
        - CONVENIENCE_W3 * df["waiting_norm"]
    )

    final_df = df[
        [
            "id",
            "current_visitors",
            "waiting",
            "rec_quality",
            "rec_balanced",
            "rec_convenience",
        ]
    ].copy()

    final_df["calculation_timestamp"] = now

    # 5. TEMP í…Œì´ë¸” ë¡œë“œ
    TEMP_TABLE = "realtime_waiting_temp"
    BACKUP_TABLE = 'realtime_waiting_old'

    final_df.to_sql(
        name=TEMP_TABLE,
        con=engine,
        schema=SCHEMA_NAME,
        if_exists="replace",
        index=False,
    )

    # 6. ì•ˆì „í•œ UPSERT (timestamp ìºìŠ¤íŒ…!)
    # Redshift íŠ¸ëœì­ì…˜ ì‹œì‘ ë° í…Œì´ë¸” ì´ë¦„ êµì²´ ì‹¤í–‰
    sql_commands = f"""
    BEGIN;

    -- 1. ê¸°ì¡´ ìµœì¢… í…Œì´ë¸”ì„ ë°±ì—… í…Œì´ë¸”ë¡œ ì´ë¦„ ë³€ê²½
    ALTER TABLE {SCHEMA_NAME}.{FINAL_TABLE_NAME} RENAME TO {BACKUP_TABLE};

    -- 2. ì„ì‹œ í…Œì´ë¸”ì„ ìµœì¢… í…Œì´ë¸” ì´ë¦„ìœ¼ë¡œ ë³€ê²½ (ì›ìì  êµì²´)
    ALTER TABLE {SCHEMA_NAME}.{TEMP_TABLE} RENAME TO {FINAL_TABLE_NAME};

    COMMIT;

    -- 3. ì´ì „ ë²„ì „ì˜ ë°±ì—… í…Œì´ë¸” ì •ë¦¬
    DROP TABLE IF EXISTS {SCHEMA_NAME}.{BACKUP_TABLE};
    """


    redshift_hook.run(sql_commands)

    print("*ver2_04_calculate_realtime_scores.py*\n"
          f"âœ… realtime_waiting UPSERT ì™„ë£Œ: {len(final_df)} rows")
    payload = {"text": (f"ğŸ“Œ *âœ… realtime_waiting UPSERT ì™„ë£Œ: {len(final_df)} rows*\n")}

    requests.post(
        SLACK_WEBHOOK_URL,
        json=payload,
        timeout=10,
    )

# =========================
# DAG ì •ì˜
# =========================
default_args = {
    "owner": "airflow",
    "start_date": datetime(2025, 1, 1),
    "retries": 1,
    "retry_delay": timedelta(minutes=2),
}

with DAG(
    dag_id="ver2_04_calculate_realtime_scores",
    default_args=default_args,
    schedule=None,  # ë˜ëŠ” ì›í•˜ëŠ” ì£¼ê¸°
    catchup=False,
) as dag:

    t0_create_table = SQLExecuteQueryOperator(
        task_id="create_realtime_table_if_not_exists",
        conn_id=REDSHIFT_CONN_ID,
        sql=REALTIME_TABLE_CREATE_SQL,
    )

    t1_calculate = PythonOperator(
        task_id="calculate_and_upsert_realtime_scores",
        python_callable=calculate_realtime_scores,
    )

    # # ğŸ’¡ ì¶”ê°€: ë‹¤ìŒ DAGë¥¼ ì‹¤í–‰ì‹œí‚¤ëŠ” íƒœìŠ¤í¬
    # t2_trigger_map_search = TriggerDagRunOperator(
    #     task_id="trigger_map_search_dag",
    #     trigger_dag_id="map_search", # ì‹¤í–‰ì‹œí‚¤ê³ ì í•˜ëŠ” ìƒëŒ€ DAGì˜ dag_id
    #     wait_for_completion=False, # ì´ DAGê°€ ìƒëŒ€ë°©ì´ ëë‚  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦´ì§€ ì—¬ë¶€
    #     poke_interval=60
    # )

    # íŒŒì´í”„ë¼ì¸ íë¦„ ì—…ë°ì´íŠ¸
    # t0_create_table >> t1_calculate >> t2_trigger_map_search
    t0_create_table >> t1_calculate
