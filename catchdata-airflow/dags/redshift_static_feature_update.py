import json
import math
from datetime import datetime, timedelta

import pandas as pd
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.common.sql.operators.sql import (
    SQLExecuteQueryOperator,  # í…Œì´ë¸” ìƒì„±ìš©
)
from airflow.providers.postgres.hooks.postgres import PostgresHook
from sqlalchemy import Numeric

# =========================
# ê¸°ë³¸ ì„¤ì •
# =========================
REDSHIFT_CONN_ID = "redshift_conn"
SCHEMA_NAME = "analytics"
RAW_TABLE = "raw_data.kakao_crawl"
FINAL_TABLE_NAME = "derived_features_base"

# ê°€ì¤‘ì¹˜ ì„¤ì • (base_population ê³„ì‚°ìš©)
W_REVIEW = 1.0
W_BLOG = 0.7

# 24ê°œ ì‹œê°„ëŒ€ ì»¬ëŸ¼ ì´ë¦„ ì •ì˜
TIME_COLUMNS = [f'time{i}' for i in range(24)]

# =========================
# ðŸ’¡ SQL: ìµœì¢… í…Œì´ë¸” ìƒì„± ìŠ¤í‚¤ë§ˆ
# =========================
# Redshiftì— ìµœì í™”ëœ í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ ì •ì˜
FINAL_TABLE_CREATE_SQL = f"""
CREATE TABLE IF NOT EXISTS {SCHEMA_NAME}.{FINAL_TABLE_NAME} (
    id VARCHAR(256) PRIMARY KEY,
    base_population NUMERIC(18, 4),
    quality_score NUMERIC(18, 4),
    rating NUMERIC(3, 2),
    -- 24ê°œ ì‹œê°„ëŒ€ ì»¬ëŸ¼ (ë°©ë¬¸ìž ìˆ˜ëŠ” ìž‘ìœ¼ë¯€ë¡œ SMALLINT ì‚¬ìš©)
    {', '.join([f'{col} SMALLINT' for col in TIME_COLUMNS])},
    calculated_at TIMESTAMP
)
-- idë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë°ì´í„° ë¶„ì‚° ë° ì •ë ¬í•˜ì—¬ ì¡°ì¸ ë° ì¿¼ë¦¬ ì„±ëŠ¥ ìµœì í™”
DISTKEY(id) 
SORTKEY(calculated_at);
"""

# =========================
# ðŸ’¡ ë‹¨ì¼ í†µí•© í•¨ìˆ˜: ëª¨ë“  ë¡œì§ì„ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰ (Atomic Replacement)
# =========================
def full_static_feature_pipeline():
    """
    hourly_visit JSONì„ 24ê°œ ì»¬ëŸ¼ìœ¼ë¡œ ë³€í™˜í•˜ê³ , 
    Redshift í…Œì´ë¸” ì´ë¦„ êµì²´ë¥¼ í†µí•´ ì›ìžì ìœ¼ë¡œ ê°±ì‹ í•©ë‹ˆë‹¤.
    """

    # Redshift Hook ì´ˆê¸°í™”
    redshift_hook = PostgresHook(postgres_conn_id=REDSHIFT_CONN_ID)
    engine = redshift_hook.get_sqlalchemy_engine()

    # 1. Redshiftì—ì„œ ì›ë³¸ ë°ì´í„° ë¡œë“œ
    print("--- 1. Redshiftì—ì„œ ì›ë³¸ ë°ì´í„° ë¡œë“œ ì‹œìž‘ ---")
    sql_select = f"""
    SELECT 
        id, 
        rating, 
        review_count, 
        blog_count, 
        hourly_visit
    FROM {RAW_TABLE};
    """

    df = redshift_hook.get_pandas_df(sql_select)

    if df.empty:
        print(f"ê²½ê³ : {RAW_TABLE} í…Œì´ë¸”ì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. íŒŒì´í”„ë¼ì¸ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return

    print(f"âœ… ì›ë³¸ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(df)}ê°œ")


    # 2. íŒŒìƒ ë³€ìˆ˜ ê³„ì‚° ë° hourly_visit ë¶„ë¦¬ (Python/Pandas í™˜ê²½)
    print("--- 2. íŒŒìƒ ë³€ìˆ˜ ê³„ì‚° ë° hourly_visit ë¶„ë¦¬ ì‹œìž‘ ---")

    # --- base_population ê³„ì‚° ---
    df['base_population'] = (
        df['review_count'].apply(math.log1p) * W_REVIEW +
        df['blog_count'].apply(math.log1p) * W_BLOG
    )

    # --- quality_score ê³„ì‚° ---
    df['quality_score'] = df['base_population'] * df['rating'].astype(float, errors='ignore')

    # --- hourly_visit JSON íŒŒì‹± ë° 24ê°œ ì»¬ëŸ¼ ë¶„ë¦¬ ---
    def safe_loads(json_str):
        """JSON íŒŒì‹± ì¤‘ ì˜¤ë¥˜ ë°œìƒ ì‹œ 0ìœ¼ë¡œ ì±„ì›Œì§„ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜"""
        try:
            if pd.isna(json_str) or json_str is None:
                return [0] * 24
            return json.loads(json_str)
        except Exception:
            # ë¦¬ìŠ¤íŠ¸ ê¸¸ì´ê°€ 24ê°€ ì•„ë‹Œ ê²½ìš°ì—ë„ 0ìœ¼ë¡œ ì±„ì›Œì§„ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
            return [0] * 24

    df['hourly_list'] = df['hourly_visit'].apply(safe_loads)

    # 24ê°œ ì‹œê°„ëŒ€ë³„ ì»¬ëŸ¼ ìƒì„±
    # Redshiftì˜ ìž‘ì€ ì •ìˆ˜í˜•(SMALLINT)ìœ¼ë¡œ ì €ìž¥í•˜ê¸° ìœ„í•´ íƒ€ìž… ë³€í™˜
    df[TIME_COLUMNS] = pd.DataFrame(df['hourly_list'].to_list(), index=df.index).astype('int16')
    df.drop(columns=['hourly_list', 'hourly_visit'], inplace=True)


    # --- ìµœì¢… í…Œì´ë¸” êµ¬ì¡° ì¤€ë¹„ ---
    final_df = df[[
        'id',
        'base_population',
        'quality_score',
        'rating',
        *TIME_COLUMNS
    ]].copy()

    final_df['calculated_at'] = datetime.now()

    print("âœ… íŒŒìƒ ë³€ìˆ˜ ë° ì‹œê°„ëŒ€ ì»¬ëŸ¼ ê³„ì‚° ì™„ë£Œ")


    # 3. Redshift í…Œì´ë¸” ì´ë¦„ ë³€ê²½ì„ í†µí•œ ì›ìžì  êµì²´
    print("--- 3. Redshift í…Œì´ë¸” ì´ë¦„ êµì²´ ì‹œìž‘ (Atomic Replacement) ---")

    # ðŸ’¡ ìž„ì‹œ í…Œì´ë¸” ë° ë°±ì—… í…Œì´ë¸” ì´ë¦„ ì •ì˜
    STAGING_TABLE = 'derived_features_staging'
    BACKUP_TABLE = 'derived_features_old'

    # ðŸ’¡ ë°ì´í„° íƒ€ìž… ë§¤í•‘ ì •ì˜ (Redshift SMALLINTë¡œ ë§¤í•‘í•˜ê¸° ìœ„í•´ ëª…ì‹œ)
    dtype_mapping = {
        'base_population': Numeric(18, 4),
        'quality_score': Numeric(18, 4),
        'rating': Numeric(3, 2),
        # TIME_COLUMNSì˜ íƒ€ìž…ì€ int16ì„ í†µí•´ SMALLINTë¡œ ìžë™ìœ¼ë¡œ ì¶”ë¡ ë˜ë„ë¡ í•©ë‹ˆë‹¤.
    }

    # 3-1. ê³„ì‚°ëœ final_dfë¥¼ ìž„ì‹œ Staging í…Œì´ë¸”ì— ë¡œë“œ
    final_df.to_sql(
        name=STAGING_TABLE,
        con=engine,
        schema=SCHEMA_NAME,
        if_exists='replace',
        index=False,
        dtype=dtype_mapping
    )

    print(f"   -> Staging í…Œì´ë¸” ë¡œë“œ ì™„ë£Œ: {SCHEMA_NAME}.{STAGING_TABLE}")


    # 3-2. Redshift íŠ¸ëžœìž­ì…˜ ì‹œìž‘ ë° í…Œì´ë¸” ì´ë¦„ êµì²´ ì‹¤í–‰
    sql_commands = f"""
    BEGIN;

    -- 1. ê¸°ì¡´ ìµœì¢… í…Œì´ë¸”ì„ ë°±ì—… í…Œì´ë¸”ë¡œ ì´ë¦„ ë³€ê²½
    ALTER TABLE {SCHEMA_NAME}.{FINAL_TABLE_NAME} RENAME TO {BACKUP_TABLE};

    -- 2. ìž„ì‹œ í…Œì´ë¸”ì„ ìµœì¢… í…Œì´ë¸” ì´ë¦„ìœ¼ë¡œ ë³€ê²½ (ì›ìžì  êµì²´)
    ALTER TABLE {SCHEMA_NAME}.{STAGING_TABLE} RENAME TO {FINAL_TABLE_NAME};

    COMMIT;

    -- 3. ì´ì „ ë²„ì „ì˜ ë°±ì—… í…Œì´ë¸” ì •ë¦¬
    DROP TABLE IF EXISTS {SCHEMA_NAME}.{BACKUP_TABLE};
    """


    redshift_hook.run(sql_commands)

    print(f"âœ… {SCHEMA_NAME}.{FINAL_TABLE_NAME} í…Œì´ë¸”ì´ {len(final_df)}ê°œ ë ˆì½”ë“œë¡œ ì„œë¹„ìŠ¤ ì¤‘ë‹¨ ì—†ì´ ê°±ì‹ ë˜ì—ˆìŠµë‹ˆë‹¤.")


# =========================
# DAG ì •ì˜
# =========================
default_args = {
    "owner": "airflow",
    "start_date": datetime(2025, 1, 1),
    "retries": 1,
    "retry_delay": timedelta(minutes=5)
}

with DAG(
    dag_id="redshift_static_feature_update",
    default_args=default_args,
    description="hourly_visitì„ 24ê°œ time ì»¬ëŸ¼ìœ¼ë¡œ ë¶„ë¦¬í•˜ê³  RENAMEì„ í†µí•´ Redshift í…Œì´ë¸”ì„ ì›ìžì ìœ¼ë¡œ ê°±ì‹ í•©ë‹ˆë‹¤.",
    schedule="@daily",
    catchup=False
) as dag:

    # T0. ìµœì¢… í…Œì´ë¸”ì´ ì—†ëŠ” ê²½ìš° ìƒì„± (ìµœì´ˆ ì‹¤í–‰ ì‹œ ì•ˆì •ì„± í™•ë³´)
    t0_create_table = SQLExecuteQueryOperator(
        task_id="create_final_table_if_not_exists",
        conn_id=REDSHIFT_CONN_ID,
        sql=FINAL_TABLE_CREATE_SQL,
    )

    # T1. ë°ì´í„° ë¡œë“œ, ê³„ì‚° ë° ìµœì¢… í…Œì´ë¸” ê°±ì‹ 
    t1_full_pipeline = PythonOperator(
        task_id="run_full_static_feature_pipeline",
        python_callable=full_static_feature_pipeline,
    )

    # íŒŒì´í”„ë¼ì¸ íë¦„ ì •ì˜: í…Œì´ë¸” ìƒì„± í™•ì¸ í›„ ë°ì´í„° ê°±ì‹ 
    t0_create_table >> t1_full_pipeline
