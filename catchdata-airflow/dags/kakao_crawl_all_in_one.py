import base64
import multiprocessing
import threading
import time
from datetime import datetime, timedelta, timezone

import boto3
import pandas as pd
import requests
from airflow import DAG
from airflow.operators.python import PythonOperator

# ChromeDriver ë‹¤ìš´ë¡œë“œ Lock (ë™ì‹œ ë‹¤ìš´ë¡œë“œ ë°©ì§€)
_driver_lock = threading.Lock()


# =========================
#  ê¸°ë³¸ ì„¤ì •
# =========================
REST_API_KEY = ""
SLACK_WEBHOOK_URL = ("https://hooks.slack.com/services/T09SZ0BSHEU"
                     "/B0A3W3R4H9D/Ea5DqrFBnQKc3SzbSuNhcmZo")
KST = timezone(timedelta(hours=9))
time_stamp = datetime.now(KST).strftime("%Y%m%d")
BUCKET_NAME = "427paul-test-bucket"
OUTPUT_KEY = f"kakao_crawl/eating_house_{time_stamp}.csv"


# =========================
# í¬ë¡¤ë§ í•¨ìˆ˜
# =========================
def crawl_kakao_place(place_url):
    import time

    import cv2
    import numpy as np
    from selenium import webdriver
    from selenium.webdriver.chrome.service import Service
    from selenium.webdriver.common.by import By
    from selenium.webdriver.support import expected_conditions as EC
    from selenium.webdriver.support.ui import WebDriverWait
    from webdriver_manager.chrome import ChromeDriverManager

    options = webdriver.ChromeOptions()
    options.add_argument("--headless=new")
    options.add_argument("--disable-gpu")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-blink-features=AutomationControlled")
    options.add_argument("--window-size=1280,800")
    options.add_argument("user-agent=Mozilla/5.0")

    # Lockì„ ì‚¬ìš©í•˜ì—¬ ChromeDriver ë‹¤ìš´ë¡œë“œ ë™ì‹œì„± ë¬¸ì œ ë°©ì§€
    with _driver_lock:
        driver_path = ChromeDriverManager().install()

    driver = webdriver.Chrome(
        service=Service(driver_path),
        options=options
    )

    wait = WebDriverWait(driver, 10)

    driver.get(place_url)
    time.sleep(1.0)

    # ë°©ë¬¸ìž ê·¸ëž˜í”„ ì´ë¯¸ì§€ ì²˜ë¦¬
    img_values = None
    try:
        canvas = wait.until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "div.view_chart canvas"))
        )
        img_base64 = driver.execute_script(
            "return arguments[0].toDataURL('image/png').substring(22);",
            canvas
        )
        img_data = base64.b64decode(img_base64)
        img = cv2.imdecode(np.frombuffer(img_data, np.uint8), cv2.IMREAD_COLOR)
        h, w, _ = img.shape
        hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)
        mask = cv2.inRange(hsv, np.array([90, 40, 40]), np.array([250, 180, 255]))
        values = []
        x_positions = [int((i + 0.5) * w / 24) for i in range(24)]
        for x in x_positions:
            ys = np.where(mask[:, x] > 0)[0]
            values.append(
                round((h - ys[0]) / h * 100, 1) if len(ys) else np.nan
            )
        clean = np.array(values)
        idx = np.arange(24)
        if np.any(~np.isnan(clean)):
            clean[np.isnan(clean)] = np.interp(
                idx[np.isnan(clean)], idx[~np.isnan(clean)], clean[~np.isnan(clean)]
            )
        img_values = clean.tolist()
    except:
        img_values = [0] * 24

    # ë³„ì 
    try:
        rating = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "span.num_star"))).text
    except:
        rating = 0

    # í›„ê¸° & ë¸”ë¡œê·¸ ìˆ˜
    review_cnt = 0
    blog_cnt = 0
    try:
        titles = wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, "span.info_tit")))
        counts = wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, "span.info_num")))
        title_list = [t.text for t in titles]
        count_list = [c.text for c in counts]
        if "í›„ê¸°" in title_list:
            review_cnt = count_list[title_list.index("í›„ê¸°")]
        if "ë¸”ë¡œê·¸" in title_list:
            blog_cnt = count_list[title_list.index("ë¸”ë¡œê·¸")]
    except:
        pass


    # ì´ë¯¸ì§€ URL
    img_url = None
    try:
        # ì‚¬ì§„ ëª©ë¡ ì˜ì—­
        container = wait.until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "div.inner_board"))
        )

        imgs = container.find_elements(By.TAG_NAME, "img")

        for img in imgs:
            src = img.get_attribute("src")
            if src and src.startswith("http"):
                img_url = src   # âœ… ì²« ë²ˆì§¸ ì´ë¯¸ì§€ ë°œê²¬ ì¦‰ì‹œ ë°˜í™˜
                break
    except:
        pass

    driver.quit()

    return {
        "rating": rating,
        "review_count": review_cnt,
        "blog_count": blog_cnt,
        "hourly_visit": img_values,
        "img_url":img_url,
        "waiting": 0,
        "update_time": time.strftime("%Y-%m-%d")
    }


def process_row(row):
    # place_url = f"https://place.map.kakao.com/{row['id']}"
    return crawl_kakao_place(row['place_url'])


# =========================
# í†µí•© ìž‘ì—… í•¨ìˆ˜
# =========================
def run_all_tasks(**context):
    """
    1. Kakao APIë¡œ ìŒì‹ì  ëª©ë¡ ìˆ˜ì§‘
    2. ë³‘ë ¬ í¬ë¡¤ë§ìœ¼ë¡œ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘
    3. S3ì— ê²°ê³¼ ì—…ë¡œë“œ
    """

    # ========================================
    # TASK 1: Kakao API ëª©ë¡ ìˆ˜ì§‘
    # ========================================
    print("=" * 60)
    print("ðŸ”Ž TASK 1 ì‹œìž‘: Kakao API ìŒì‹ì  ëª©ë¡ ìˆ˜ì§‘")
    print("=" * 60)

    url = "https://dapi.kakao.com/v2/local/search/keyword.json"
    headers = {"Authorization": f"KakaoAK {REST_API_KEY}"}

    all_results = []

    query = "í™ëŒ€ ìŒì‹ì "
    for page in range(1, 3):
        params = {
            "query": query,
            "size": 15,
            "page": page
        }

        res = requests.get(url, params=params, headers=headers).json()
        docs = res.get("documents", [])

        if not docs:
            break

        all_results.extend(docs)
        time.sleep(0.3)

    query = "ëŒ€ì¹˜ë™ ìŒì‹ì "

    for page in range(1, 3):
        params = {
            "query": query,
            "size": 15,
            "page": page
        }

        res = requests.get(url, params=params, headers=headers).json()
        docs = res.get("documents", [])

        if not docs:
            break

        all_results.extend(docs)
        time.sleep(0.3)

    df = pd.DataFrame(all_results)


    # ì£¼ì†Œ í•„í„° - ì„œìš¸ ë§ˆí¬êµ¬ë§Œ
    # df = df[df["address_name"].str.startswith("ì„œìš¸ ë§ˆí¬êµ¬")]

    # ìŒì‹ì ë§Œ (FD6)
    df = df[df["category_group_code"] == "FD6"]
    
    # full_static_feature_pipeline í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ df ë¡œë“œ ì§í›„ ì‹¤í–‰
    before_drop = len(df)
    print(f"ì „ì²˜ë¦¬ ì „ ë°ì´í„° ìˆ˜: {before_drop}")

    # idë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ ì œê±° (ì²« ë²ˆì§¸ ë°ì´í„°ë§Œ ë‚¨ê¹€)
    df = df.drop_duplicates(subset=['id'], keep='first')
    after_drop = len(df)
    print(f"ì „ì²˜ë¦¬ í›„ ë°ì´í„° ìˆ˜: {after_drop}")

    print(f"âœ… TASK 1 ì™„ë£Œ: ì´ {after_drop}ê°œ ìŒì‹ì  ëª©ë¡ ìˆ˜ì§‘ ì™„ë£Œ")
    print("=" * 60)
    print()
    
    payload = {"text": (f"ðŸ“Œ *kakao_crawl_all_on_one.py*\n"
                        f"ì´ {before_drop}ê°œ ìŒì‹ì  ì¤‘ ì „ì²˜ë¦¬ í›„ {after_drop} ëª©ë¡ ìˆ˜ì§‘ ì™„ë£Œ*\n")}
    requests.post(
        SLACK_WEBHOOK_URL,
        json=payload,
        timeout=10,
    )
    
    # ========================================
    # TASK 2: ë³‘ë ¬ í¬ë¡¤ë§ìœ¼ë¡œ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘
    # ========================================
    print("=" * 60)
    print("ðŸ•·ï¸ TASK 2 ì‹œìž‘: ìŒì‹ì  ìƒì„¸ ì •ë³´ ë³‘ë ¬ í¬ë¡¤ë§")
    print("=" * 60)

    # ChromeDriver ë¯¸ë¦¬ ë‹¤ìš´ë¡œë“œ (ë™ì‹œ ë‹¤ìš´ë¡œë“œ ë°©ì§€)
    print("ChromeDriver ë‹¤ìš´ë¡œë“œ ì¤‘...")
    from webdriver_manager.chrome import ChromeDriverManager
    driver_path = ChromeDriverManager().install()
    print(f"ChromeDriver ì¤€ë¹„ ì™„ë£Œ: {driver_path}")

    from concurrent.futures import ThreadPoolExecutor, as_completed

    workers = min(4, multiprocessing.cpu_count())
    print(f"ë³‘ë ¬ ì²˜ë¦¬ ì›Œì»¤ ìˆ˜: {workers}")

    results = []
    tasks = []

    with ThreadPoolExecutor(max_workers=workers) as executor:
        for i, row in df.iterrows():
            tasks.append(executor.submit(process_row, row))

        completed = 0
        for future in as_completed(tasks):
            try:
                results.append(future.result())
                completed += 1
                if completed % 5 == 0 or completed == len(tasks):
                    print(f"ì§„í–‰ ìƒí™©: {completed}/{len(tasks)} ì™„ë£Œ")
            except Exception as e:
                print(f"í¬ë¡¤ë§ ì‹¤íŒ¨: {str(e)}")
                # ì‹¤íŒ¨í•œ ê²½ìš° ë¹ˆ ë°ì´í„° ì¶”ê°€
                results.append({
                    "rating": 0,
                    "review_count": 0,
                    "blog_count": 0,
                    "hourly_visit": [0] * 24,
                    "img_url" : "None",
                    "waiting": 0,
                    "update_time": time_stamp
                })
                completed += 1

    # distance, place_url ì»¬ëŸ¼ ì œê±°
    df = df.drop(columns=["distance", "place_url"], errors="ignore")

    final_df = pd.concat([df.reset_index(drop=True), pd.DataFrame(results)], axis=1)
    before_drop = len(final_df)
    
    # idë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ ì œê±° (ì²« ë²ˆì§¸ ë°ì´í„°ë§Œ ë‚¨ê¹€)
    final_df = final_df.drop_duplicates(subset=['id'], keep='first')
    after_drop = len(final_df)
    
    payload = {"text": (f"ðŸ“Œ *kakao_crawl_all_on_one.py*\n"
                        f"í¬ë¡¤ë§ {before_drop}ê°œ ìŒì‹ì  ëª©ë¡ ìˆ˜ì§‘ ì™„ë£Œ\n"
                        f"ì „ì²˜ë¦¬ í›„ {after_drop}ê°œ ìŒì‹ì  ëª©ë¡ S3 ì ìž¬ ì‹œìž‘\n")}
    requests.post(
        SLACK_WEBHOOK_URL,
        json=payload,
        timeout=10,
    )
    
    
    print(f"âœ… TASK 2 ì™„ë£Œ: ì´ {len(final_df)}ê°œ ìŒì‹ì  í¬ë¡¤ë§ ì™„ë£Œ")
    print("=" * 60)
    print(final_df.head())
    print("=" * 60)
    print()

    # ========================================
    # TASK 3: S3ì— ê²°ê³¼ ì—…ë¡œë“œ
    # ========================================
    print("=" * 60)
    print("â˜ï¸ TASK 3 ì‹œìž‘: S3ì— ê²°ê³¼ ì—…ë¡œë“œ")
    print("=" * 60)

    s3 = boto3.client(
        "s3"
    )

    # UTF-8 BOM ì¶”ê°€ë¡œ í•œê¸€ ê¹¨ì§ ë°©ì§€ (Excelì—ì„œë„ ì •ìƒ í‘œì‹œ)
    csv_buffer = final_df.to_csv(index=False, encoding='utf-8-sig')

    s3.put_object(
        Bucket=BUCKET_NAME,
        Key=OUTPUT_KEY,
        Body=csv_buffer.encode("utf-8-sig"),
        ContentType="text/csv; charset=utf-8"
    )

    print("âœ… TASK 3 ì™„ë£Œ: S3 ì—…ë¡œë“œ ì„±ê³µ")
    print(f"ðŸ“ ì €ìž¥ ìœ„ì¹˜: s3://{BUCKET_NAME}/{OUTPUT_KEY}")
    print(f"ðŸ“Š ì—…ë¡œë“œëœ ë°ì´í„°: {len(final_df)}í–‰, {len(final_df.columns)}ì—´")
    print("=" * 60)
    print()
    print("ðŸŽ‰ ì „ì²´ ìž‘ì—… ì™„ë£Œ!")
    payload = {"text": ("*kakao_crawl_all_in_one.py*\n"
        f"ðŸ“Œ kakao_crawl/eating_house_{time_stamp}.csv ì—…ë¡œë“œ ì™„ë£Œ\n"
                        f"ì´ {len(final_df)}ê°œ ë°ì´í„° S3 ì ìž¬ ì™„ë£Œ")}

    requests.post(
        SLACK_WEBHOOK_URL,
        json=payload,
        timeout=10,
    )


# =========================
# DAG ì •ì˜
# =========================

from airflow.operators.trigger_dagrun import TriggerDagRunOperator

default_args = {
    "owner": "ê·œì˜",
    "email_on_failure": False,
    "retries": 1,
    "retry_delay": timedelta(minutes=2)
}

with DAG(
    dag_id="kakao_crawl_all_in_one",
    start_date=datetime(2025, 1, 1),
    schedule="0 3 * * 1", # ë§¤ì£¼ ì›”ìš”ì¼ 03:00 ì‹¤í–‰
    catchup=False,
    default_args=default_args
):

    run_all = PythonOperator(
        task_id="run_all_tasks",
        python_callable=run_all_tasks
    )

    trigger_load_redshift = TriggerDagRunOperator(
        task_id="trigger_load_s3_to_redshift",
        trigger_dag_id="load_s3_to_redshift",
        wait_for_completion=False,
        reset_dag_run=False
    )

    # run_all ëë‚˜ë©´ extract_kakao_url DAG ì‹¤í–‰ë¨
    run_all >> trigger_load_redshift
    # run_all

