import os
import time
import base64
import pandas as pd
import multiprocessing
import threading

from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta, timezone

import requests
import boto3

# ChromeDriver ë‹¤ìš´ë¡œë“œ Lock (ë™ì‹œ ë‹¤ìš´ë¡œë“œ ë°©ì§€)
_driver_lock = threading.Lock()


# =========================
#  ê¸°ë³¸ ì„¤ì •
# =========================
AWS_ACCESS_KEY = "AWS_ACCESS_KEY"
AWS_SECRET_KEY = "AWS_SEVCRET_KEY"

KST = timezone(timedelta(hours=9))
time_stamp = datetime.now(KST).strftime("%Y%m%d")
BUCKET_NAME = "427paul-test-bucket"
OUTPUT_KEY = f"kakao_crawl/eating_house_{time_stamp}.csv"


# =========================
# í¬ë¡¤ë§ í•¨ìˆ˜
# =========================
def crawl_kakao_place(place_url):
    import time
    import base64
    from selenium import webdriver
    from selenium.webdriver.chrome.service import Service
    from selenium.webdriver.common.by import By
    from webdriver_manager.chrome import ChromeDriverManager
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC
    import numpy as np
    import cv2
    
    options = webdriver.ChromeOptions()
    options.add_argument("--headless=new")
    options.add_argument("--disable-gpu")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-blink-features=AutomationControlled")
    options.add_argument("--window-size=1280,800")
    options.add_argument("user-agent=Mozilla/5.0")
    
    # Lockì„ ì‚¬ìš©í•˜ì—¬ ChromeDriver ë‹¤ìš´ë¡œë“œ ë™ì‹œì„± ë¬¸ì œ ë°©ì§€
    with _driver_lock:
        driver_path = ChromeDriverManager().install()

    driver = webdriver.Chrome(
        service=Service(driver_path),
        options=options
    )

    wait = WebDriverWait(driver, 10)

    driver.get(place_url)
    time.sleep(1.0)

    # ë°©ë¬¸ìž ê·¸ëž˜í”„ ì´ë¯¸ì§€ ì²˜ë¦¬
    img_values = None
    try:
        canvas = wait.until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "div.view_chart canvas"))
        )
        img_base64 = driver.execute_script(
            "return arguments[0].toDataURL('image/png').substring(22);",
            canvas
        )
        img_data = base64.b64decode(img_base64)
        img = cv2.imdecode(np.frombuffer(img_data, np.uint8), cv2.IMREAD_COLOR)
        h, w, _ = img.shape
        hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)
        mask = cv2.inRange(hsv, np.array([90, 40, 40]), np.array([250, 180, 255]))
        values = []
        x_positions = [int((i + 0.5) * w / 24) for i in range(24)]
        for x in x_positions:
            ys = np.where(mask[:, x] > 0)[0]
            values.append(
                round((h - ys[0]) / h * 100, 1) if len(ys) else np.nan
            )
        clean = np.array(values)
        idx = np.arange(24)
        if np.any(~np.isnan(clean)):
            clean[np.isnan(clean)] = np.interp(
                idx[np.isnan(clean)], idx[~np.isnan(clean)], clean[~np.isnan(clean)]
            )
        img_values = clean.tolist()
    except:
        img_values = [0] * 24

    # ë³„ì 
    try:
        rating = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "span.num_star"))).text
    except:
        rating = 0

    # í›„ê¸° & ë¸”ë¡œê·¸ ìˆ˜
    review_cnt = 0
    blog_cnt = 0
    try:
        titles = wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, "span.info_tit")))
        counts = wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, "span.info_num")))
        title_list = [t.text for t in titles]
        count_list = [c.text for c in counts]
        if "í›„ê¸°" in title_list:
            review_cnt = count_list[title_list.index("í›„ê¸°")]
        if "ë¸”ë¡œê·¸" in title_list:
            blog_cnt = count_list[title_list.index("ë¸”ë¡œê·¸")]
    except:
        pass

        
    # ì´ë¯¸ì§€ URL
    img_url = None
    try:
        # ì‚¬ì§„ ëª©ë¡ ì˜ì—­
        container = wait.until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "div.inner_board"))
        )

        imgs = container.find_elements(By.TAG_NAME, "img")

        for img in imgs:
            src = img.get_attribute("src")
            if src and src.startswith("http"):
                img_url = src   # âœ… ì²« ë²ˆì§¸ ì´ë¯¸ì§€ ë°œê²¬ ì¦‰ì‹œ ë°˜í™˜
                break
    except:
        pass

    driver.quit()

    return {
        "rating": rating,
        "review_count": review_cnt,
        "blog_count": blog_cnt,
        "hourly_visit": img_values,
        "img_url":img_url,
        "waiting": 0,
        "update_time": time.strftime("%Y-%m-%d")
    }


def process_row(row):
    # place_url = f"https://place.map.kakao.com/{row['id']}"
    return crawl_kakao_place(row['place_url'])


# =========================
# í†µí•© ìž‘ì—… í•¨ìˆ˜
# =========================
def run_all_tasks(**context):
    """
    1. Kakao APIë¡œ ìŒì‹ì  ëª©ë¡ ìˆ˜ì§‘
    2. ë³‘ë ¬ í¬ë¡¤ë§ìœ¼ë¡œ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘
    3. S3ì— ê²°ê³¼ ì—…ë¡œë“œ
    """
    
    # ========================================
    # TASK 1: Kakao API ëª©ë¡ ìˆ˜ì§‘
    # ========================================
    print("=" * 60)
    print("ðŸ”Ž TASK 1 ì‹œìž‘: Kakao API ìŒì‹ì  ëª©ë¡ ìˆ˜ì§‘")
    print("=" * 60)
    
    REST_API_KEY = "REST_API_KEY"
    url = "https://dapi.kakao.com/v2/local/search/keyword.json"
    headers = {"Authorization": f"KakaoAK {REST_API_KEY}"}
    
    all_results = []

    query = "í™ëŒ€ ìŒì‹ì "
    for page in range(1, 2):
        params = {
            "query": query,
            "size": 15,
            "page": page
        }

        res = requests.get(url, params=params, headers=headers).json()
        docs = res.get("documents", [])

        if not docs:
            break

        all_results.extend(docs)
        time.sleep(0.3)
        
    query = "ëŒ€ì¹˜ë™ ìŒì‹ì "
        
    for page in range(1, 2):
        params = {
            "query": query,
            "size": 15,
            "page": page
        }

        res = requests.get(url, params=params, headers=headers).json()
        docs = res.get("documents", [])

        if not docs:
            break
        
        all_results.extend(docs)
        time.sleep(0.3)
        
    df = pd.DataFrame(all_results)

    
    # ì£¼ì†Œ í•„í„° - ì„œìš¸ ë§ˆí¬êµ¬ë§Œ
    # df = df[df["address_name"].str.startswith("ì„œìš¸ ë§ˆí¬êµ¬")]
    
    # ìŒì‹ì ë§Œ (FD6)
    df = df[df["category_group_code"] == "FD6"]
    
    print(f"âœ… TASK 1 ì™„ë£Œ: ì´ {len(df)}ê°œ ìŒì‹ì  ëª©ë¡ ìˆ˜ì§‘ ì™„ë£Œ")
    print("=" * 60)
    print()
    
    # ========================================
    # TASK 2: ë³‘ë ¬ í¬ë¡¤ë§ìœ¼ë¡œ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘
    # ========================================
    print("=" * 60)
    print("ðŸ•·ï¸ TASK 2 ì‹œìž‘: ìŒì‹ì  ìƒì„¸ ì •ë³´ ë³‘ë ¬ í¬ë¡¤ë§")
    print("=" * 60)
    
    # ChromeDriver ë¯¸ë¦¬ ë‹¤ìš´ë¡œë“œ (ë™ì‹œ ë‹¤ìš´ë¡œë“œ ë°©ì§€)
    print("ChromeDriver ë‹¤ìš´ë¡œë“œ ì¤‘...")
    from selenium import webdriver
    from webdriver_manager.chrome import ChromeDriverManager
    driver_path = ChromeDriverManager().install()
    print(f"ChromeDriver ì¤€ë¹„ ì™„ë£Œ: {driver_path}")
    
    from concurrent.futures import ThreadPoolExecutor, as_completed
    
    workers = min(4, multiprocessing.cpu_count())
    print(f"ë³‘ë ¬ ì²˜ë¦¬ ì›Œì»¤ ìˆ˜: {workers}")
    
    results = []
    tasks = []

    with ThreadPoolExecutor(max_workers=workers) as executor:
        for i, row in df.iterrows():
            tasks.append(executor.submit(process_row, row))
        
        completed = 0
        for future in as_completed(tasks):
            try:
                results.append(future.result())
                completed += 1
                if completed % 5 == 0 or completed == len(tasks):
                    print(f"ì§„í–‰ ìƒí™©: {completed}/{len(tasks)} ì™„ë£Œ")
            except Exception as e:
                print(f"í¬ë¡¤ë§ ì‹¤íŒ¨: {str(e)}")
                # ì‹¤íŒ¨í•œ ê²½ìš° ë¹ˆ ë°ì´í„° ì¶”ê°€
                results.append({
                    "rating": 0,
                    "review_count": 0,
                    "blog_count": 0,
                    "hourly_visit": [0] * 24,
                    "img_url" : "None",
                    "waiting": 0,
                    "update_time": time_stamp
                })
                completed += 1

    # distance, place_url ì»¬ëŸ¼ ì œê±°
    df = df.drop(columns=["distance", "place_url"], errors="ignore")
    
    final_df = pd.concat([df.reset_index(drop=True), pd.DataFrame(results)], axis=1)
    print(f"âœ… TASK 2 ì™„ë£Œ: ì´ {len(final_df)}ê°œ ìŒì‹ì  í¬ë¡¤ë§ ì™„ë£Œ")
    print("=" * 60)
    print(final_df.head())
    print("=" * 60)
    print()
    
    # ========================================
    # TASK 3: S3ì— ê²°ê³¼ ì—…ë¡œë“œ
    # ========================================
    print("=" * 60)
    print("â˜ï¸ TASK 3 ì‹œìž‘: S3ì— ê²°ê³¼ ì—…ë¡œë“œ")
    print("=" * 60)
    
    s3 = boto3.client(
        "s3",
        aws_access_key_id=AWS_ACCESS_KEY,
        aws_secret_access_key=AWS_SECRET_KEY
    )

    # UTF-8 BOM ì¶”ê°€ë¡œ í•œê¸€ ê¹¨ì§ ë°©ì§€ (Excelì—ì„œë„ ì •ìƒ í‘œì‹œ)
    csv_buffer = final_df.to_csv(index=False, encoding='utf-8-sig')

    s3.put_object(
        Bucket=BUCKET_NAME,
        Key=OUTPUT_KEY,
        Body=csv_buffer.encode("utf-8-sig"),
        ContentType="text/csv; charset=utf-8"
    )

    print(f"âœ… TASK 3 ì™„ë£Œ: S3 ì—…ë¡œë“œ ì„±ê³µ")
    print(f"ðŸ“ ì €ìž¥ ìœ„ì¹˜: s3://{BUCKET_NAME}/{OUTPUT_KEY}")
    print(f"ðŸ“Š ì—…ë¡œë“œëœ ë°ì´í„°: {len(final_df)}í–‰, {len(final_df.columns)}ì—´")
    print("=" * 60)
    print()
    print("ðŸŽ‰ ì „ì²´ ìž‘ì—… ì™„ë£Œ!")


# =========================
# DAG ì •ì˜
# =========================

from airflow.operators.trigger_dagrun import TriggerDagRunOperator


default_args = {
    "owner": "ê·œì˜",
    "email_on_failure": False,
    "retries": 1,
    "retry_delay": timedelta(minutes=2)
}

with DAG(
    dag_id="kakao_crawl_all_in_one",
    start_date=datetime(2025, 1, 1),
    schedule="0 3 * * 1", # ë§¤ì£¼ ì›”ìš”ì¼ 03:00 ì‹¤í–‰
    catchup=False,
    default_args=default_args
):

    run_all = PythonOperator(
        task_id="run_all_tasks",
        python_callable=run_all_tasks
    )
    
    trigger_load_redshift = TriggerDagRunOperator(
        task_id="trigger_load_s3_to_redshift",
        trigger_dag_id="load_s3_to_redshift",
        wait_for_completion=False,
        reset_dag_run=False
    )

    # run_all ëë‚˜ë©´ extract_kakao_url DAG ì‹¤í–‰ë¨
    run_all >> trigger_load_redshift
    # run_all