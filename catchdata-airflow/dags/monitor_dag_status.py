    from airflow import DAG
    from airflow.providers.standard.operators.python import PythonOperator
    from airflow.providers.postgres.hooks.postgres import PostgresHook
    from airflow.models import Variable
    from datetime import datetime, timedelta
    import requests

    # =========================
    # ì„¤ì •ê°’
    # =========================
    SLACK_WEBHOOK_URL = Variable.get("SLACK_WEBHOOK_URL")

    CHECK_INTERVAL_MIN = 60        # ìµœê·¼ 1ì‹œê°„
    RUNNING_THRESHOLD_MIN = 30    # 30ë¶„ ì´ìƒ running

    # =========================
    # ëª¨ë‹ˆí„°ë§ ë¡œì§
    # =========================
    def monitor_dags():
        hook = PostgresHook(postgres_conn_id="airflow_db")
        conn = hook.get_conn()
        cur = conn.cursor()

        # 1ï¸âƒ£ ì‹¤íŒ¨í•œ DAG (dag_run â†’ logical_date)
        cur.execute(f"""
            SELECT dag_id, logical_date
            FROM dag_run
            WHERE state = 'failed'
            AND logical_date >= NOW() - INTERVAL '{CHECK_INTERVAL_MIN} minutes'
            ORDER BY logical_date DESC
        """)
        failed_dags = cur.fetchall()

        # 2ï¸âƒ£ ì‹¤íŒ¨í•œ Task (task_instance â†’ start_date)
        cur.execute(f"""
            SELECT dag_id, task_id, start_date
            FROM task_instance
            WHERE state = 'failed'
            AND start_date >= NOW() - INTERVAL '{CHECK_INTERVAL_MIN} minutes'
            ORDER BY start_date DESC
        """)
        failed_tasks = cur.fetchall()

        # 3ï¸âƒ£ ì¥ì‹œê°„ running DAG (dag_run â†’ start_date)
        cur.execute(f"""
            SELECT dag_id, start_date
            FROM dag_run
            WHERE state = 'running'
            AND start_date <= NOW() - INTERVAL '{RUNNING_THRESHOLD_MIN} minutes'
            ORDER BY start_date
        """)
        long_running_dags = cur.fetchall()

        cur.close()
        conn.close()

        if not failed_dags and not failed_tasks and not long_running_dags:
            return  # ì•Œë¦¼ ë³´ë‚¼ ê²Œ ì—†ìœ¼ë©´ ì¢…ë£Œ

        # =========================
        # Slack ë©”ì‹œì§€ êµ¬ì„±
        # =========================
        message = "*ğŸš¨ Airflow DAG ëª¨ë‹ˆí„°ë§ ì•Œë¦¼*\n\n"

        if failed_dags:
            message += "âŒ *ì‹¤íŒ¨í•œ DAG (ìµœê·¼ 1ì‹œê°„)*\n"
            for dag_id, logical_date in failed_dags:
                message += f"â€¢ `{dag_id}` @ {logical_date}\n"
            message += "\n"

        if failed_tasks:
            message += "ğŸ§© *ì‹¤íŒ¨í•œ Task (ìµœê·¼ 1ì‹œê°„)*\n"
            for dag_id, task_id, start_date in failed_tasks:
                message += f"â€¢ `{dag_id}.{task_id}` @ {start_date}\n"
            message += "\n"

        if long_running_dags:
            message += "ğŸ•’ *30ë¶„ ì´ìƒ ì‹¤í–‰ ì¤‘ì¸ DAG*\n"
            for dag_id, start_date in long_running_dags:
                message += f"â€¢ `{dag_id}` (ì‹œì‘: {start_date})\n"
            message += "\n"

        # =========================
        # Slack ì „ì†¡
        # =========================
        resp = requests.post(
            SLACK_WEBHOOK_URL,
            json={"text": message},
            timeout=10
        )

        if resp.status_code != 200:
            raise RuntimeError(f"Slack webhook failed: {resp.text}")

    # =========================
    # DAG ì •ì˜
    # =========================
    default_args = {
        "owner": "airflow",
        "retries": 1,
        "retry_delay": timedelta(minutes=2),
    }

    with DAG(
        dag_id="dag_monitoring",
        description="Airflow DAG ìƒíƒœ ëª¨ë‹ˆí„°ë§ (ì‹¤íŒ¨ / ì¥ê¸° ì‹¤í–‰)",
        start_date=datetime(2025, 1, 1),
        schedule="*/10 * * * *",  # 10ë¶„ë§ˆë‹¤
        catchup=False,
        default_args=default_args,
        tags=["monitoring", "slack"],
    ) as dag:

        monitor_task = PythonOperator(
            task_id="monitor_dag_status",
            python_callable=monitor_dags,
        )
