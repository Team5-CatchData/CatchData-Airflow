from airflow import DAG
from airflow.providers.standard.operators.python import PythonOperator
from airflow.providers.postgres.hooks.postgres  import PostgresHook
from datetime import datetime, timedelta
import requests

# =========================
# ì„¤ì •ê°’
# =========================
SLACK_WEBHOOK_URL = "{{ var.value.SLACK_WEBHOOK_URL }}"

AIRFLOW_BASE_URL = "http://localhost:18080"

CHECK_INTERVAL_MIN = 60        # ìµœê·¼ 1ì‹œê°„
RUNNING_THRESHOLD_MIN = 30    # 30ë¶„ ì´ìƒ running

# =========================
# ëª¨ë‹ˆí„°ë§ ë¡œì§
# =========================
def monitor_dags():
    hook = PostgresHook(postgres_conn_id="airflow_db")
    conn = hook.get_conn()
    cur = conn.cursor()

    # ì‹¤íŒ¨í•œ DAG
    cur.execute(f"""
        SELECT dag_id, logical_date
        FROM dag_run
        WHERE state = 'failed'
        AND logical_date >= NOW() - INTERVAL '{CHECK_INTERVAL_MIN} minutes'
        ORDER BY logical_date DESC
    """)
    failed_dags = cur.fetchall()

    # ì‹¤íŒ¨í•œ Task
    cur.execute(f"""
        SELECT dag_id, task_id, logical_date
        FROM task_instance
        WHERE state = 'failed'
        AND logical_date >= NOW() - INTERVAL '{CHECK_INTERVAL_MIN} minutes'
        ORDER BY logical_date DESC
    """)
    failed_tasks = cur.fetchall()
    # ì¥ì‹œê°„ running DAG
    cur.execute(f"""
        SELECT dag_id, logical_date
        FROM dag_run
        WHERE state = 'running'
        AND logical_date <= NOW() - INTERVAL '{RUNNING_THRESHOLD_MIN} minutes'
        ORDER BY logical_date
    """)
    long_running_dags = cur.fetchall()

    cur.close()
    conn.close()

    if not failed_dags and not failed_tasks and not long_running_dags:
        return  # ì•Œë¦¼ ë³´ë‚¼ ê²Œ ì—†ìœ¼ë©´ ì¢…ë£Œ

    # =========================
    # Slack ë©”ì‹œì§€ êµ¬ì„±
    # =========================
    message = "*ğŸš¨ Airflow DAG ëª¨ë‹ˆí„°ë§ ì•Œë¦¼*\n\n"

    if failed_dags:
        message += "âŒ *ì‹¤íŒ¨í•œ DAG (ìµœê·¼ 1ì‹œê°„)*\n"
        for dag_id, exec_date in failed_dags:
            message += f"â€¢ `{dag_id}` @ {exec_date}\n"
        message += "\n"

    if failed_tasks:
        message += "ğŸ§© *ì‹¤íŒ¨í•œ Task (ìµœê·¼ 1ì‹œê°„)*\n"
        for dag_id, task_id, exec_date in failed_tasks:
            message += f"â€¢ `{dag_id}.{task_id}` @ {exec_date}\n"
        message += "\n"

    if long_running_dags:
        message += "ğŸ•’ *30ë¶„ ì´ìƒ ì‹¤í–‰ ì¤‘ì¸ DAG*\n"
        for dag_id, exec_date in long_running_dags:
            message += f"â€¢ `{dag_id}` (ì‹œì‘: {exec_date})\n"
        message += "\n"

    # DAG Grid ë§í¬ (ì¤‘ë³µ ì œê±°)
    target_dag_ids = set([d[0] for d in failed_dags + long_running_dags])
    
    if target_dag_ids:
        message += "ğŸ”— *DAG Grid ë°”ë¡œê°€ê¸°*\n"
        for dag_id in target_dag_ids:
            message += f"â€¢ <{AIRFLOW_BASE_URL}/dags/{dag_id}/grid|{dag_id} Grid>\n"


    requests.post(
        SLACK_WEBHOOK_URL,
        json={"text": message},
        timeout=10
    )

# =========================
# DAG ì •ì˜
# =========================
default_args = {
    "owner": "airflow",
    "retries": 1,
    "retry_delay": timedelta(minutes=2)
}

with DAG(
    dag_id="dag_monitoring",
    description="Airflow DAG ìƒíƒœ ëª¨ë‹ˆí„°ë§ (ì‹¤íŒ¨ / ì¥ê¸° ì‹¤í–‰)",
    start_date=datetime(2025, 1, 1),
    schedule="*/10 * * * *",  # 10ë¶„ë§ˆë‹¤
    catchup=False,
    default_args=default_args,
    tags=["monitoring", "slack"]
) as dag:

    monitor_task = PythonOperator(
        task_id="monitor_dag_status",
        python_callable=monitor_dags
    )
