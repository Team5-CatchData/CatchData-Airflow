import logging
from datetime import datetime, timedelta

from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.postgres.hooks.postgres import PostgresHook


def transfer_redshift_to_rds(**context):
    """Redshift â†’ RDS ì „ì†¡ (UPSERT ë°©ì‹)"""
    
    redshift_hook = PostgresHook(postgres_conn_id="redshift_conn")
    rds_hook = PostgresHook(postgres_conn_id="rds_conn")
    
    # 1. Redshiftì—ì„œ ë°ì´í„° ì¶”ì¶œ
    logging.info("1. Redshift ë°ì´í„° ì¶”ì¶œ")
    sql = """
        SELECT 
            id, name, region, city, category, rating, 
            phone, x, y, waiting, image_url, address,
            rec_quality, rec_balanced, rec_convenience
        FROM analytics.map_search
        ORDER BY id
    """
    
    records = redshift_hook.get_records(sql)
    record_count = len(records)
    logging.info(f"âœ“ {record_count:,}ê°œ ì¶”ì¶œ ì™„ë£Œ")
    
    if not records:
        logging.warning("ì¶”ì¶œëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
        return
    
    # 2. RDSì— UPSERT
    logging.info(f"2. RDS UPSERT ì‹œì‘ ({record_count:,}ê°œ)")
    
    conn = rds_hook.get_conn()
    cursor = conn.cursor()
    # postgresqlì€ ëŒ€ì†Œë¬¸ì êµ¬ë³„ ëª»í•¨
    # ë”°ë¼ì„œ ì»¬ëŸ¼ëª…ì„ ""ë¡œ ê°ì‹¸ì¤˜ì•¼ ëŒ€ì†Œë¬¸ì êµ¬ë³„ ê°€ëŠ¥
    try:
        cursor.executemany("""
            INSERT INTO main_restaurant (
                "restaurant_ID", name, region, city, category, rating, phone, x, y, waiting,
                image_url, address, rec_quality, rec_balanced, rec_convenience
            ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
            ON CONFLICT ("restaurant_ID")
            DO UPDATE SET
                name = EXCLUDED.name,
                region = EXCLUDED.region,
                city = EXCLUDED.city,
                category = EXCLUDED.category,
                rating = EXCLUDED.rating,
                phone = EXCLUDED.phone,
                x = EXCLUDED.x,
                y = EXCLUDED.y,
                waiting = EXCLUDED.waiting,
                image_url = EXCLUDED.image_url,
                address = EXCLUDED.address,
                rec_quality = EXCLUDED.rec_quality,
                rec_balanced = EXCLUDED.rec_balanced,
                rec_convenience = EXCLUDED.rec_convenience
        """, records)
        
        conn.commit()
        logging.info(f"âœ“ {record_count:,}ê°œ UPSERT ì™„ë£Œ!")
        
    except Exception as e:
        conn.rollback()
        logging.error(f"âœ— UPSERT ì‹¤íŒ¨: {e}")
        raise
    finally:
        cursor.close()
        conn.close()


# DAG ì •ì˜
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2024, 12, 19),
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

with DAG(
    dag_id='redshift_to_rds_transfer',
    default_args=default_args,
    description='Redshift â†’ RDS ë°ì´í„° ì „ì†¡ (UPSERT ë°©ì‹)',
    schedule='30 3 * * 1',
    catchup=False,
    tags=['redshift', 'rds', 'upsert'],
) as dag:
    
    transfer_task = PythonOperator(
        task_id='transfer_data',
        python_callable=transfer_redshift_to_rds,
    )


# import logging
# from datetime import datetime, timedelta

# from airflow import DAG
# from airflow.operators.python import PythonOperator
# from airflow.providers.postgres.hooks.postgres import PostgresHook


# def transfer_redshift_to_rds(**context):
#     """Redshift â†’ RDS ì „ì†¡ (UPSERT ë°©ì‹)"""
    
#     redshift_hook = PostgresHook(postgres_conn_id="redshift_conn")
#     rds_hook = PostgresHook(postgres_conn_id="rds_conn")
    
#     # 1. Redshiftì—ì„œ ë°ì´í„° ì¶”ì¶œ
#     logging.info("1. Redshift ë°ì´í„° ì¶”ì¶œ")
    
#     # Redshift ì»¬ëŸ¼ ìˆœì„œì— ë§ì¶°ì„œ SELECT
#     sql = """
#         SELECT 
#             id,                 -- 1
#             name,               -- 2
#             region,             -- 3
#             city,               -- 4
#             category,           -- 5
#             rating,             -- 6
#             phone,              -- 7
#             x,                  -- 8
#             y,                  -- 9
#             image_url,          -- 10
#             address,            -- 11
#             rec_quality,        -- 12
#             rec_balanced,       -- 13
#             rec_convenience     -- 14
#         FROM analytics.map_search
#         ORDER BY id
#         LIMIT 10
#     """
    
#     records = redshift_hook.get_records(sql)
#     record_count = len(records)
#     logging.info(f"âœ“ {record_count:,}ê°œ ì¶”ì¶œ ì™„ë£Œ")
    
#     # ë””ë²„ê¹…
#     if records:
#         first = records[0]
#         logging.info(f"ğŸ” ì²« ë ˆì½”ë“œ ê¸¸ì´: {len(first)}ê°œ")
#         logging.info(f"ğŸ” ì²« ë ˆì½”ë“œ: {first}")
    
#     if not records:
#         logging.warning("ì¶”ì¶œëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
#         return
    
#     # 2. RDSì— UPSERT
#     logging.info(f"2. RDS UPSERT ì‹œì‘ ({record_count:,}ê°œ)")
    
#     conn = rds_hook.get_conn()
#     cursor = conn.cursor()
    
#     try:
#         # RDS ì»¬ëŸ¼ ìˆœì„œì— ë§ì¶°ì„œ INSERT (14ê°œ)
#         cursor.executemany("""
#             INSERT INTO main_restaurant (
#                 "restaurant_ID",
#                 name,
#                 region,
#                 city,
#                 category,
#                 rating,
#                 phone,
#                 x,
#                 y,
#                 waiting,            
#                 image_url,
#                 address,
#                 rec_quality,
#                 rec_balanced,
#                 rec_convenience
#             ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
#             ON CONFLICT ("restaurant_ID")
#             DO UPDATE SET
#                 name = EXCLUDED.name,
#                 region = EXCLUDED.region,
#                 city = EXCLUDED.city,
#                 category = EXCLUDED.category,
#                 rating = EXCLUDED.rating,
#                 phone = EXCLUDED.phone,
#                 x = EXCLUDED.x,
#                 y = EXCLUDED.y,
#                 waiting = EXCLUDED.waiting,
#                 image_url = EXCLUDED.image_url,
#                 address = EXCLUDED.address,
#                 rec_quality = EXCLUDED.rec_quality,
#                 rec_balanced = EXCLUDED.rec_balanced,
#                 rec_convenience = EXCLUDED.rec_convenience
#         """, records)
        
#         conn.commit()
#         logging.info(f"âœ“ {record_count:,}ê°œ UPSERT ì™„ë£Œ!")
        
#     except Exception as e:
#         conn.rollback()
#         logging.error(f"âœ— UPSERT ì‹¤íŒ¨: {e}")
#         logging.error(f"ğŸ” ì—ëŸ¬ ë°œìƒí•œ ë ˆì½”ë“œ: {records[0] if records else 'None'}")
#         raise
#     finally:
#         cursor.close()
#         conn.close()


# # DAG ì •ì˜
# default_args = {
#     'owner': 'airflow',
#     'depends_on_past': False,
#     'start_date': datetime(2024, 12, 19),
#     'retries': 1,
#     'retry_delay': timedelta(minutes=5),
# }

# with DAG(
#     dag_id='redshift_to_rds_transfer',
#     default_args=default_args,
#     description='Redshift â†’ RDS ë°ì´í„° ì „ì†¡ (UPSERT ë°©ì‹)',
#     schedule='30 3 * * 1',
#     catchup=False,
#     tags=['redshift', 'rds', 'upsert'],
# ) as dag:
    
#     transfer_task = PythonOperator(
#         task_id='transfer_data',
#         python_callable=transfer_redshift_to_rds,
#     )