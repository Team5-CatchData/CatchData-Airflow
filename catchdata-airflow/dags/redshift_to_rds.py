# import logging
# from datetime import datetime, timedelta

# from airflow import DAG
# from airflow.operators.python import PythonOperator
# # from airflow.sensors.external_task import ExternalTaskSensor
# from airflow.providers.postgres.hooks.postgres import PostgresHook
# # from airflow.utils.state import DagRunState


# def transfer_redshift_to_rds(**context):
#     """Redshift â†’ RDS ì „ì†¡ (UPSERT ë°©ì‹)"""
    
#     redshift_hook = PostgresHook(postgres_conn_id="redshift_conn")
#     rds_hook = PostgresHook(postgres_conn_id="rds_conn")
    
#     # 1. Redshiftì—ì„œ ë°ì´í„° ì¶”ì¶œ
#     logging.info("1. Redshift ë°ì´í„° ì¶”ì¶œ")
#     sql = """
#         SELECT 
#             id, name, region, city, category, rating, 
#             phone, x, y, image_url, address,
#             rec_quality, rec_balanced, rec_convenience
#         FROM analytics.map_search
#         ORDER BY id
#         LIMIT 10
#     """
    
#     records = redshift_hook.get_records(sql)
#     record_count = len(records)
#     logging.info(f"âœ“ {record_count:,}ê°œ ì¶”ì¶œ ì™„ë£Œ")
    
#     if not records:
#         logging.warning("ì¶”ì¶œëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
#         return
    
#     # 2. RDSì— UPSERT
#     logging.info(f"2. RDS UPSERT ì‹œì‘ ({record_count:,}ê°œ)")
    
#     conn = rds_hook.get_conn()
#     cursor = conn.cursor()
    
#     try:
#         cursor.executemany("""
#             INSERT INTO main_restaurant (
#                 restaurant_ID, name, region, city, category, rating, phone, x, y,
#                 image_url, address, rec_quality, rec_balanced, rec_convenience, waitting
#             ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
#             ON CONFLICT (restaurant_ID) 
#             DO UPDATE SET
#                 name = EXCLUDED.name,
#                 region = EXCLUDED.region,
#                 city = EXCLUDED.city,
#                 category = EXCLUDED.category,
#                 rating = EXCLUDED.rating,
#                 phone = EXCLUDED.phone,
#                 x = EXCLUDED.x,
#                 y = EXCLUDED.y,
#                 waitting = EXCLUDED.waitting,
#                 image_url = EXCLUDED.image_url,
#                 address = EXCLUDED.address,
#                 rec_quality = EXCLUDED.rec_quality,
#                 rec_balanced = EXCLUDED.rec_balanced,
#                 rec_convenience = EXCLUDED.rec_convenience
#         """, records)
        
#         conn.commit()
#         logging.info(f"âœ“ {record_count:,}ê°œ UPSERT ì™„ë£Œ!")
        
#     except Exception as e:
#         conn.rollback()
#         logging.error(f"âœ— UPSERT ì‹¤íŒ¨: {e}")
#         raise
#     finally:
#         cursor.close()
#         conn.close()


# # DAG ì •ì˜
# default_args = {
#     'owner': 'airflow',
#     'depends_on_past': False,
#     'start_date': datetime(2024, 12, 19),
#     'retries': 1,
#     'retry_delay': timedelta(minutes=5),
# }

# with DAG(
#     dag_id='redshift_to_rds_transfer',
#     default_args=default_args,
#     description='Redshift â†’ RDS ë°ì´í„° ì „ì†¡ (UPSERT ë°©ì‹)',
#     schedule='30 3 * * 1',  # ë§¤ì£¼ ì›”ìš”ì¼ ìƒˆë²½ 3ì‹œ 30ë¶„
#     catchup=False,
#     tags=['redshift', 'rds', 'upsert'],
# ) as dag:
    
#     # # ìƒìœ„ DAG ì™„ë£Œ ëŒ€ê¸°
#     # wait_for_pipeline = ExternalTaskSensor(
#     #     task_id='wait_for_redshift_pipeline',
#     #     external_dag_id='redshift_map_search_update_pipeline',
#     #     external_task_id=None,  # DAG ì „ì²´ ì™„ë£Œ ëŒ€ê¸°
#     #     allowed_states=[DagRunState.SUCCESS],
#     #     failed_states=[DagRunState.FAILED],
#     #     execution_delta=timedelta(hours=0),
#     #     poke_interval=60,
#     #     timeout=3600,
#     #     mode='poke',
#     # )
    
#     # ë°ì´í„° ì „ì†¡
#     transfer_task = PythonOperator(
#         task_id='transfer_data',
#         python_callable=transfer_redshift_to_rds,
#     )
    
#     # ì˜ì¡´ì„±
#     transfer_task


import logging
from datetime import datetime, timedelta

from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.postgres.hooks.postgres import PostgresHook


def transfer_redshift_to_rds(**context):
    """Redshift â†’ RDS ì „ì†¡ (UPSERT ë°©ì‹)"""
    
    redshift_hook = PostgresHook(postgres_conn_id="redshift_conn")
    rds_hook = PostgresHook(postgres_conn_id="rds_conn")
    
    # 1. Redshiftì—ì„œ ë°ì´í„° ì¶”ì¶œ
    logging.info("1. Redshift ë°ì´í„° ì¶”ì¶œ")
    
    # â­ Redshift ì»¬ëŸ¼ ìˆœì„œì— ë§ì¶°ì„œ SELECT
    sql = """
        SELECT 
            id,                 -- 1
            name,               -- 2
            region,             -- 3
            city,               -- 4
            category,           -- 5
            rating,             -- 6
            phone,              -- 7
            x,                  -- 8
            y,                  -- 9
            image_url,          -- 10
            address,            -- 11
            rec_quality,        -- 12
            rec_balanced,       -- 13
            rec_convenience     -- 14
        FROM analytics.map_search
        ORDER BY id
        LIMIT 10
    """
    
    records = redshift_hook.get_records(sql)
    record_count = len(records)
    logging.info(f"âœ“ {record_count:,}ê°œ ì¶”ì¶œ ì™„ë£Œ")
    
    # ë””ë²„ê¹…
    if records:
        first = records[0]
        logging.info(f"ğŸ” ì²« ë ˆì½”ë“œ ê¸¸ì´: {len(first)}ê°œ")
        logging.info(f"ğŸ” ì²« ë ˆì½”ë“œ: {first}")
    
    if not records:
        logging.warning("ì¶”ì¶œëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
        return
    
    # 2. RDSì— UPSERT
    logging.info(f"2. RDS UPSERT ì‹œì‘ ({record_count:,}ê°œ)")
    
    conn = rds_hook.get_conn()
    cursor = conn.cursor()
    
    try:
        # â­ RDS ì»¬ëŸ¼ ìˆœì„œì— ë§ì¶°ì„œ INSERT (14ê°œ)
        cursor.executemany("""
            INSERT INTO main_restaurant (
                "restaurant_ID",    -- 1 â† Redshift id
                name,               -- 2 â† Redshift name
                region,             -- 3 â† Redshift region
                city,               -- 4 â† Redshift city
                category,           -- 5 â† Redshift category
                rating,             -- 6 â† Redshift rating
                phone,              -- 7 â† Redshift phone
                x,                  -- 8 â† Redshift x
                y,                  -- 9 â† Redshift y
                image_url,          -- 10 â† Redshift image_url
                address,            -- 11 â† Redshift address
                rec_quality,        -- 12 â† Redshift rec_quality
                rec_balanced,       -- 13 â† Redshift rec_balanced
                rec_convenience     -- 14 â† Redshift rec_convenience
            ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
            ON CONFLICT ("restaurant_ID")
            DO UPDATE SET
                name = EXCLUDED.name,
                region = EXCLUDED.region,
                city = EXCLUDED.city,
                category = EXCLUDED.category,
                rating = EXCLUDED.rating,
                phone = EXCLUDED.phone,
                x = EXCLUDED.x,
                y = EXCLUDED.y,
                image_url = EXCLUDED.image_url,
                address = EXCLUDED.address,
                rec_quality = EXCLUDED.rec_quality,
                rec_balanced = EXCLUDED.rec_balanced,
                rec_convenience = EXCLUDED.rec_convenience
        """, records)
        
        conn.commit()
        logging.info(f"âœ“ {record_count:,}ê°œ UPSERT ì™„ë£Œ!")
        
    except Exception as e:
        conn.rollback()
        logging.error(f"âœ— UPSERT ì‹¤íŒ¨: {e}")
        logging.error(f"ğŸ” ì—ëŸ¬ ë°œìƒí•œ ë ˆì½”ë“œ: {records[0] if records else 'None'}")
        raise
    finally:
        cursor.close()
        conn.close()


# DAG ì •ì˜
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2024, 12, 19),
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

with DAG(
    dag_id='redshift_to_rds_transfer',
    default_args=default_args,
    description='Redshift â†’ RDS ë°ì´í„° ì „ì†¡ (UPSERT ë°©ì‹)',
    schedule='30 3 * * 1',
    catchup=False,
    tags=['redshift', 'rds', 'upsert'],
) as dag:
    
    transfer_task = PythonOperator(
        task_id='transfer_data',
        python_callable=transfer_redshift_to_rds,
    )