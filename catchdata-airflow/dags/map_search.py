import pandas as pd
from datetime import datetime, timedelta

from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.common.sql.operators.sql import SQLExecuteQueryOperator
from airflow.providers.postgres.hooks.postgres import PostgresHook
from sqlalchemy import Numeric, String

# =========================
# ê¸°ë³¸ ì„¤ì •
# =========================
REDSHIFT_CONN_ID = "redshift_conn"
SCHEMA_NAME = "analytics"
RAW_TABLE1 = "raw_data.kakao_crawl"
RAW_TABLE2 = "analytics.realtime_waiting"
FINAL_TABLE_NAME = "map_search"

# =========================
# ðŸ’¡ SQL: ìµœì¢… í…Œì´ë¸” ìƒì„± ìŠ¤í‚¤ë§ˆ
# =========================
# SQL ë¬¸ë²• ì˜¤ë¥˜(ì½¤ë§ˆ) ìˆ˜ì • ë° VARCHAR ê¸¸ì´ ìµœì í™”
FINAL_TABLE_CREATE_SQL = f"""
CREATE TABLE IF NOT EXISTS {SCHEMA_NAME}.{FINAL_TABLE_NAME} (
    name VARCHAR(50),
    region VARCHAR(50),
    city VARCHAR(50),
    category VARCHAR(100),
    x FLOAT,
    y FLOAT,
    waiting INTEGER,
    rating FLOAT,
    phone VARCHAR(50),
    image_url VARCHAR(500),
    address VARCHAR(300),
    rec_quality FLOAT,
    rec_balanced FLOAT,
    rec_convenience FLOAT
) DISTSTYLE EVEN;
"""

def full_static_feature_pipeline():
    """
    ì›ë³¸ ë° ì‹¤ì‹œê°„ í…Œì´ë¸”ì„ ì¡°ì¸í•˜ì—¬ ê²€ìƒ‰ ì „ìš© í…Œì´ë¸”ì„ ìƒì„±í•˜ê³  ì›ìžì ìœ¼ë¡œ ê°±ì‹ í•©ë‹ˆë‹¤.
    """

    # Redshift Hook ì´ˆê¸°í™”
    redshift_hook = PostgresHook(postgres_conn_id=REDSHIFT_CONN_ID)
    engine = redshift_hook.get_sqlalchemy_engine()

    # 1. Redshiftì—ì„œ í†µí•© ë°ì´í„° ë¡œë“œ
    # category_nameì„ ê°€ì ¸ì™€ì„œ íŒŒì´ì¬ì—ì„œ categoryë¡œ ê°€ê³µí•  ì˜ˆì •
    print("--- 1. Redshiftì—ì„œ ì›ë³¸ ë°ì´í„° ë¡œë“œ ì‹œìž‘ ---")
    sql_select = f"""
    SELECT 
        A.place_name as name,
        A.address_name as address,
        A.category_name,  -- ì›ë³¸ ì¹´í…Œê³ ë¦¬ ë¡œë“œ
        A.x,
        A.y,
        B.waiting,
        A.rating,
        A.phone,
        A.img_url as image_url,
        B.rec_quality,
        B.rec_balanced,
        B.rec_convenience
    FROM {RAW_TABLE1} A
    INNER JOIN {RAW_TABLE2} B 
        ON CAST(A.id AS VARCHAR) = B.id;
    """

    df = redshift_hook.get_pandas_df(sql_select)

    if df.empty:
        print("ê²½ê³ : ì›ë³¸ í…Œì´ë¸”ì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. íŒŒì´í”„ë¼ì¸ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return

    print(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(df)}ê°œ")

    # 2. ë°ì´í„° ì „ì²˜ë¦¬ (Python/Pandas í™˜ê²½)
    print("--- 2. ë°ì´í„° ì „ì²˜ë¦¬ ì‹œìž‘ ---")
    
    # 2-1. category ê°€ê³µ (ë§ˆì§€ë§‰ ìš”ì†Œ ì¶”ì¶œ)
    df['category'] = df['category_name'].str.split('>').str[-1].str.strip()

    # 2-2. address ë¶„ë¦¬ (region, city)
    # n=2ë¥¼ ì£¼ì–´ ì²« ë‘ ë‹¨ì–´ë§Œ ë¶„ë¦¬í•˜ê³  ë‚˜ë¨¸ì§€ëŠ” ìœ ì§€
    address_split = df['address'].str.split(' ', n=2, expand=True)
    df['region'] = address_split[0]
    df['city'] = address_split[1]

    # 2-3. ìµœì¢… í…Œì´ë¸” ì»¬ëŸ¼ ìˆœì„œ ë° êµ¬ì„± í™•ì •
    final_df = df[['name', 'region', 'city', 'category', 'x', 'y', 
                   'waiting', 'rating', 'phone', 'image_url', 'address',
                   'rec_quality', 'rec_balanced', 'rec_convenience']].copy()

    print("âœ… ì „ì²˜ë¦¬ ì™„ë£Œ")

    # 3. Redshift ì›ìžì  êµì²´ ì‹¤í–‰
    STAGING_TABLE = f"{FINAL_TABLE_NAME}_staging"
    BACKUP_TABLE = f"{FINAL_TABLE_NAME}_old"

    # 3-1. Staging í…Œì´ë¸”ì— ë¡œë“œ
    # ë°ì´í„° íƒ€ìž…ì„ ëª…ì‹œì ìœ¼ë¡œ ì§€ì •í•˜ì—¬ Redshift ìŠ¤í‚¤ë§ˆì™€ ì¼ì¹˜ì‹œí‚´
    dtype_mapping = {
        'x': Numeric(15, 12),
        'y': Numeric(15, 12),
        'rating': Numeric(3, 1),
        'name': String(50),
        'category': String(50),
        'image_url': String(500), # ðŸš¨ ì´ ë¶€ë¶„ì´ í•µì‹¬ í•´ê²°ì±…
        'address': String(300),
        'rec_quality': Numeric(15,14),
        'rec_balanced': Numeric(15,14),
        'rec_convenience': Numeric(15,14)
    }

    print(f"--- 3. Staging í…Œì´ë¸” ë¡œë“œ: {STAGING_TABLE} ---")
    final_df.to_sql(
        name=STAGING_TABLE,
        con=engine,
        schema=SCHEMA_NAME,
        if_exists='replace',
        index=False,
        dtype=dtype_mapping
    )

    # 3-2. íŠ¸ëžœìž­ì…˜ ì‹œìž‘ (Atomic Swap)
    sql_commands = f"""
    BEGIN;
    -- 1. ê¸°ì¡´ í…Œì´ë¸” ë°±ì—… (ì¡´ìž¬í•  ë•Œë§Œ ì‹¤í–‰ë˜ë„ë¡ IF EXISTSëŠ” RENAMEì—ì„œ ì§ì ‘ ì§€ì› ì•ˆí•˜ë¯€ë¡œ ìˆ˜ë™ í™•ì¸ í•„ìš”í•˜ë‚˜
    -- t0 íƒœìŠ¤í¬ê°€ í…Œì´ë¸” ì¡´ìž¬ë¥¼ ë³´ìž¥í•¨)
    DROP TABLE IF EXISTS {SCHEMA_NAME}.{BACKUP_TABLE};
    ALTER TABLE {SCHEMA_NAME}.{FINAL_TABLE_NAME} RENAME TO {BACKUP_TABLE};

    -- 2. Stagingì„ ìµœì¢…ìœ¼ë¡œ ìŠ¹ê²©
    ALTER TABLE {SCHEMA_NAME}.{STAGING_TABLE} RENAME TO {FINAL_TABLE_NAME};
    COMMIT;

    -- 3. ë°±ì—… ì •ë¦¬
    DROP TABLE IF EXISTS {SCHEMA_NAME}.{BACKUP_TABLE};
    """

    redshift_hook.run(sql_commands)
    print(f"âœ… {SCHEMA_NAME}.{FINAL_TABLE_NAME} ê°±ì‹  ì™„ë£Œ")

# =========================
# DAG ì •ì˜
# =========================
default_args = {
    "owner": "airflow",
    "start_date": datetime(2025, 1, 1),
    "retries": 1,
    "retry_delay": timedelta(minutes=5)
}

with DAG(
    dag_id="redshift_map_search_update_pipeline",
    default_args=default_args,
    description="ì¹´ì¹´ì˜¤ ì›ë³¸ ë°ì´í„°ì™€ ì‹¤ì‹œê°„ ëŒ€ê¸° ë°ì´í„°ë¥¼ í•©ì³ ì§€ë„ ê²€ìƒ‰ìš© í…Œì´ë¸” ìƒì„±",
    schedule=None,
    catchup=False
) as dag:

    # T0. í…Œì´ë¸” ìƒì„± ë³´ìž¥
    t0_create_table = SQLExecuteQueryOperator(
        task_id="create_final_table_if_not_exists",
        conn_id=REDSHIFT_CONN_ID,
        sql=FINAL_TABLE_CREATE_SQL,
    )

    # T1. ë©”ì¸ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    t1_full_pipeline = PythonOperator(
        task_id="run_full_static_feature_pipeline",
        python_callable=full_static_feature_pipeline,
    )

    t0_create_table >> t1_full_pipeline