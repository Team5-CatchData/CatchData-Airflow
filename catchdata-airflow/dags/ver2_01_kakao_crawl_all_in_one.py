import base64
import multiprocessing
import threading
import time
from datetime import datetime, timedelta, timezone

import boto3
import pandas as pd
import requests
from airflow.providers.standard.operators.python import PythonOperator
from airflow.sdk import DAG, Variable

# ChromeDriver ë‹¤ìš´ë¡œë“œ Lock (ë™ì‹œ ë‹¤ìš´ë¡œë“œ ë°©ì§€)
_driver_lock = threading.Lock()


# =========================
#  ê¸°ë³¸ ì„¤ì •
# =========================
REST_API_KEY = Variable.get("KAKAO_REST_API_KEY")
SLACK_WEBHOOK_URL = Variable.get("SLACK_WEBHOOK_URL")
KST = timezone(timedelta(hours=9))
time_stamp = datetime.now(KST).strftime("%Y%m%d")
BUCKET_NAME = Variable.get("S3_BUCKET_NAME", default_var="427paul-test-bucket")
OUTPUT_KEY = f"kakao_crawl/eating_house_{time_stamp}.csv"


# =========================
# í¬ë¡¤ë§ í•¨ìˆ˜
# =========================
def crawl_kakao_place(id):
    import time

    import cv2
    import numpy as np
    from selenium import webdriver
    from selenium.webdriver.chrome.service import Service
    from selenium.webdriver.common.by import By
    from selenium.webdriver.support import expected_conditions as EC
    from selenium.webdriver.support.ui import WebDriverWait
    from webdriver_manager.chrome import ChromeDriverManager

    options = webdriver.ChromeOptions()
    options.add_argument("--headless=new")
    options.add_argument("--disable-gpu")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-blink-features=AutomationControlled")
    options.add_argument("--window-size=1280,800")
    options.add_argument("user-agent=Mozilla/5.0")

    # Lockì„ ì‚¬ìš©í•˜ì—¬ ChromeDriver ë‹¤ìš´ë¡œë“œ ë™ì‹œì„± ë¬¸ì œ ë°©ì§€

    with _driver_lock:
        driver_path = ChromeDriverManager().install()

    driver = webdriver.Chrome(
        service=Service(driver_path),
        options=options
    )

    wait = WebDriverWait(driver, 10)
    place_url = f"https://place.map.kakao.com/{id}"
    driver.get(place_url)
    time.sleep(1.0)

    # ë°©ë¬¸ìž ê·¸ëž˜í”„ ì´ë¯¸ì§€ ì²˜ë¦¬
    img_values = None
    try:
        canvas = wait.until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "div.view_chart canvas"))
        )
        img_base64 = driver.execute_script(
            "return arguments[0].toDataURL('image/png').substring(22);",
            canvas
        )
        img_data = base64.b64decode(img_base64)
        img = cv2.imdecode(np.frombuffer(img_data, np.uint8), cv2.IMREAD_COLOR)
        h, w, _ = img.shape
        hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)
        mask = cv2.inRange(hsv, np.array([90, 40, 40]), np.array([250, 180, 255]))
        values = []
        x_positions = [int((i + 0.5) * w / 24) for i in range(24)]
        for x in x_positions:
            ys = np.where(mask[:, x] > 0)[0]
            values.append(
                round((h - ys[0]) / h * 100, 1) if len(ys) else np.nan
            )
        clean = np.array(values)
        idx = np.arange(24)
        if np.any(~np.isnan(clean)):
            clean[np.isnan(clean)] = np.interp(
                idx[np.isnan(clean)], idx[~np.isnan(clean)], clean[~np.isnan(clean)]
            )
        img_values = clean.tolist()
    except Exception as e:
        print(f"{place_url} : ë°©ë¬¸ìž ê·¸ëž˜í”„ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ì—†ì–´ ì¤‘ë‹¨í•©ë‹ˆë‹¤ ({e})")
        driver.quit()
        return None
        # img_values = [0] * 24

    # ë³„ì 
    try:
        rating = wait.until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "span.num_star"))
        ).text
    except Exception:
        rating = 0

    # í›„ê¸° & ë¸”ë¡œê·¸ ìˆ˜
    review_cnt = 0
    blog_cnt = 0
    try:
        titles = wait.until(
            EC.presence_of_all_elements_located((By.CSS_SELECTOR, "span.info_tit"))
        )
        counts = wait.until(
            EC.presence_of_all_elements_located((By.CSS_SELECTOR, "span.info_num"))
        )
        title_list = [t.text for t in titles]
        count_list = [c.text for c in counts]
        if "í›„ê¸°" in title_list:
            review_cnt = count_list[title_list.index("í›„ê¸°")]
        if "ë¸”ë¡œê·¸" in title_list:
            blog_cnt = count_list[title_list.index("ë¸”ë¡œê·¸")]
    except Exception:
        pass


    # ì´ë¯¸ì§€ URL
    img_url = None
    try:
        # ì‚¬ì§„ ëª©ë¡ ì˜ì—­
        container = wait.until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "div.inner_board"))
        )

        imgs = container.find_elements(By.TAG_NAME, "img")

        for img in imgs:
            src = img.get_attribute("src")
            if src and src.startswith("http"):
                img_url = src   # âœ… ì²« ë²ˆì§¸ ì´ë¯¸ì§€ ë°œê²¬ ì¦‰ì‹œ ë°˜í™˜
                break
    except Exception:
        pass

    driver.quit()

    return {
        "id" : id,
        "rating": rating,
        "review_count": review_cnt,
        "blog_count": blog_cnt,
        "hourly_visit": img_values,
        "img_url":img_url,
        "update_time": time.strftime("%Y-%m-%d")
    }


def process_row(row):
    # place_url = f"https://place.map.kakao.com/{row['id']}"
    return crawl_kakao_place(row['id'])
    # return crawl_kakao_place(row['place_url'])


# =========================
# í†µí•© ìž‘ì—… í•¨ìˆ˜
# =========================
def run_all_tasks(**context):
    """
    1. Kakao APIë¡œ ìŒì‹ì  ëª©ë¡ ìˆ˜ì§‘
    2. ë³‘ë ¬ í¬ë¡¤ë§ìœ¼ë¡œ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘
    3. S3ì— ê²°ê³¼ ì—…ë¡œë“œ
    """

    # ========================================
    # TASK 1: Kakao API ëª©ë¡ ìˆ˜ì§‘
    # ========================================
    print("=" * 60)
    print("ðŸ”Ž TASK 1 ì‹œìž‘: Kakao API ìŒì‹ì  ëª©ë¡ ìˆ˜ì§‘")
    print("=" * 60)

    url = "https://dapi.kakao.com/v2/local/search/keyword.json"
    headers = {"Authorization": f"KakaoAK {REST_API_KEY}"}

    districts = ['í™ëŒ€', 'ëŒ€ì¹˜ë™']
    # districts = ['ì²­ë‹´ë™', 'ë¡¯ë°ì›”ë“œëª°', 'ì••êµ¬ì •', 'ì„±ìˆ˜ë™', 'ê°•ë‚¨ì—­', 'ê±´ëŒ€', 'í™ëŒ€', 'ëŒ€ì¹˜ë™']
    categories = ['í•œì‹', 'ì¼ì‹', 'ì¤‘ì‹', 'ì–‘ì‹', 'ìˆ ì§‘', 'ê³ ê¸°ì§‘',
                'ì¹˜í‚¨', 'ë¶„ì‹', 'ìƒ¤ë¸Œìƒ¤ë¸Œ', 'ê°„ì‹', 'ë·”íŽ˜'] # í‚¤ì›Œë“œ ì„¸ë¶„í™”

    all_results = []

    for loc in districts:
        print(f"\n>>> {loc} ì§€ì—­ ìˆ˜ì§‘ ì‹œìž‘...")
        district_count = 0

        for cat in categories:
            query = f"{loc} {cat}"

            for page in range(1, 2): # ê° ì„¸ë¶€ í‚¤ì›Œë“œë‹¹ 45ê°œì”© ìˆ˜ì§‘
                params = {"query": query, "size": 6, "page": page}
                res = requests.get(url, params=params, headers=headers, timeout=10).json()
                docs = res.get("documents", [])

                if not docs:
                    break

                # for doc in docs:
                #     doc['main_district'] = loc # ì–´ëŠ ì§€ì—­ì¸ì§€ ì €ìž¥
                #     doc['sub_category'] = cat  # ì–´ë–¤ í‚¤ì›Œë“œë¡œ ì°¾ì•˜ëŠ”ì§€ ì €ìž¥

                all_results.extend(docs)
                district_count += len(docs)

                if res.get("meta", {}).get("is_end"):
                    break
                time.sleep(0.1)

        print(f"{loc} ìˆ˜ì§‘ ì™„ë£Œ (ëˆ„ì : {district_count}ê°œ)")

    # ë°ì´í„°í”„ë ˆìž„ ìƒì„± ë° ì¤‘ë³µ ì œê±°
    df_final = pd.DataFrame(all_results)
    # df_final = df_final.drop_duplicates(subset=['id']).reset_index(drop=True)

    #ìˆ˜ì§‘ëœ ì´ ë°ì´í„° ìˆ˜
    crawl_data_len = len(df_final)
    print(f"\nðŸŽ‰ ì „ì²´ ìˆ˜ì§‘ ì¢…ë£Œ! ìµœì¢… ë°ì´í„°: {crawl_data_len}ê°œ")

    df = df_final
    # ìŒì‹ì ë§Œ (FD6)
    df = df[df["category_group_code"] == "FD6"]

    # ìŒì‹ì  í•„í„°ë§ í›„ ë°ì´í„° ìˆ˜
    only_FD6 = len(df)

    print(f"ìŒì‹ì  ì „ì²˜ë¦¬ í›„ ë°ì´í„° ìˆ˜: {only_FD6}")

    # idë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ ì œê±° (ì²« ë²ˆì§¸ ë°ì´í„°ë§Œ ë‚¨ê¹€)
    df = df.drop_duplicates(subset=['id'], keep='first')

    # ì¤‘ë³µ ì œê±° í›„ ë°ì´í„° ìˆ˜
    drop_duplicate = len(df)
    print(f"ì¤‘ë³µ ì œê±° í›„ ë°ì´í„° ìˆ˜: {drop_duplicate}")

    print(f"âœ… TASK 1 ì™„ë£Œ: ì´ {drop_duplicate}ê°œ ìŒì‹ì  ëª©ë¡ ìˆ˜ì§‘ ì™„ë£Œ")
    print("=" * 60)
    print()

    payload = {"text": (f"ðŸ“Œ *ver2_01_kakao_crawl_all_on_one.py*\n"
                        f"ì¹´ì¹´ì˜¤ APIì—ì„œ ì¶œë ¥ëœ ì´ ë°ì´í„° ìˆ˜ : {crawl_data_len}ê°œ\n"
                        f"ìŒì‹ì  ì „ì²˜ë¦¬ í›„ ë°ì´í„° ìˆ˜ : {only_FD6}\n"
                        f"ì¤‘ë³µ ì œê±° í›„ ë°ì´í„° ìˆ˜ : {drop_duplicate}")}
    requests.post(
        SLACK_WEBHOOK_URL,
        json=payload,
        timeout=10,
    )

    # ========================================
    # TASK 2: ë³‘ë ¬ í¬ë¡¤ë§ìœ¼ë¡œ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘
    # ========================================
    print("=" * 60)
    print("ðŸ•·ï¸ TASK 2 ì‹œìž‘: ìŒì‹ì  ìƒì„¸ ì •ë³´ ë³‘ë ¬ í¬ë¡¤ë§")
    print("=" * 60)

    # ChromeDriver ë¯¸ë¦¬ ë‹¤ìš´ë¡œë“œ (ë™ì‹œ ë‹¤ìš´ë¡œë“œ ë°©ì§€)
    print("ChromeDriver ë‹¤ìš´ë¡œë“œ ì¤‘...")
    from webdriver_manager.chrome import ChromeDriverManager
    driver_path = ChromeDriverManager().install()
    print(f"ChromeDriver ì¤€ë¹„ ì™„ë£Œ: {driver_path}")

    from concurrent.futures import ThreadPoolExecutor, as_completed

    workers = min(4, multiprocessing.cpu_count())
    print(f"ë³‘ë ¬ ì²˜ë¦¬ ì›Œì»¤ ìˆ˜: {workers}")

    results = []
    tasks = []

    with ThreadPoolExecutor(max_workers=workers) as executor:
        for _i, row in df.iterrows():
            tasks.append(executor.submit(process_row, row))

        completed = 0
        for future in as_completed(tasks):
            try:
                results.append(future.result())
                completed += 1
                if completed % 5 == 0 or completed == len(tasks):
                    print(f"ì§„í–‰ ìƒí™©: {completed}/{len(tasks)} ì™„ë£Œ")
            except Exception as e:
                print(f"í¬ë¡¤ë§ ì‹¤íŒ¨: {str(e)}")
                # ì‹¤íŒ¨í•œ ê²½ìš° ë¹ˆ ë°ì´í„° ì¶”ê°€
                results.append({
                    "id":row['id'],
                    "rating": 0,
                    "review_count": 0,
                    "blog_count": 0,
                    "hourly_visit": [0] * 24,
                    "img_url" : "None",
                    "update_time": time_stamp
                })
                completed += 1

    results = [r for r in results if r is not None]
    results_df = pd.DataFrame(results)

    final_df = pd.merge(df, results_df, on='id', how='inner')

    # distance, place_url ì»¬ëŸ¼ ì œê±°
    final_df = final_df.drop(columns=["distance", "place_url"], errors="ignore")

    before_drop = len(final_df)

    # idë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ ì œê±° (ì²« ë²ˆì§¸ ë°ì´í„°ë§Œ ë‚¨ê¹€)
    final_df = final_df.drop_duplicates(subset=['id'], keep='first')
    after_drop = len(final_df)

    payload = {"text": (f"ðŸ“Œ *ver2_01_kakao_crawl_all_on_one.py*\n"
                        f"í¬ë¡¤ë§ {before_drop}ê°œ ìŒì‹ì  ëª©ë¡ ìˆ˜ì§‘ ì™„ë£Œ\n"
                        f"ì „ì²˜ë¦¬ í›„ {after_drop}ê°œ ìŒì‹ì  ëª©ë¡ S3 ì ìž¬ ì‹œìž‘\n")}
    requests.post(
        SLACK_WEBHOOK_URL,
        json=payload,
        timeout=10,
    )


    print(f"âœ… TASK 2 ì™„ë£Œ: ì´ {len(final_df)}ê°œ ìŒì‹ì  í¬ë¡¤ë§ ì™„ë£Œ")
    print("=" * 60)
    print(final_df.head())
    print("=" * 60)
    print()

    # ========================================
    # TASK 3: S3ì— ê²°ê³¼ ì—…ë¡œë“œ
    # ========================================
    print("=" * 60)
    print("â˜ï¸ TASK 3 ì‹œìž‘: S3ì— ê²°ê³¼ ì—…ë¡œë“œ")
    print("=" * 60)

    # AWS ìžê²©ì¦ëª…ì€ Airflow Connection ë˜ëŠ” í™˜ê²½ë³€ìˆ˜ì—ì„œ ìžë™ìœ¼ë¡œ ê°€ì ¸ì˜´
    s3 = boto3.client("s3")

    # UTF-8 BOM ì¶”ê°€ë¡œ í•œê¸€ ê¹¨ì§ ë°©ì§€ (Excelì—ì„œë„ ì •ìƒ í‘œì‹œ)
    csv_buffer = final_df.to_csv(index=False, encoding='utf-8-sig')

    s3.put_object(
        Bucket=BUCKET_NAME,
        Key=OUTPUT_KEY,
        Body=csv_buffer.encode("utf-8-sig"),
        ContentType="text/csv; charset=utf-8"
    )

    print("âœ… TASK 3 ì™„ë£Œ: S3 ì—…ë¡œë“œ ì„±ê³µ")
    print(f"ðŸ“ ì €ìž¥ ìœ„ì¹˜: s3://{BUCKET_NAME}/{OUTPUT_KEY}")
    print(f"ðŸ“Š ì—…ë¡œë“œëœ ë°ì´í„°: {len(final_df)}í–‰, {len(final_df.columns)}ì—´")
    print("=" * 60)
    print()
    print("ðŸŽ‰ ì „ì²´ ìž‘ì—… ì™„ë£Œ!")
    payload = {"text": ("*ver2_01_kakao_crawl_all_in_one.py*\n"
        f"ðŸ“Œ kakao_crawl/eating_house_{time_stamp}.csv ì—…ë¡œë“œ ì™„ë£Œ\n"
                        f"ì´ {len(final_df)}ê°œ ë°ì´í„° S3 ì ìž¬ ì™„ë£Œ")}

    requests.post(
        SLACK_WEBHOOK_URL,
        json=payload,
        timeout=10,
    )


# =========================
# DAG ì •ì˜
# =========================


default_args = {
    "owner": "ê·œì˜",
    "email_on_failure": False,
    "retries": 1,
    "retry_delay": timedelta(minutes=2)
}

with DAG(
    dag_id="ver2_01_kakao_crawl_all_in_one",
    start_date=datetime(2025, 1, 1),
    schedule="0 3 * * 1", # ë§¤ì£¼ ì›”ìš”ì¼ 03:00 ì‹¤í–‰
    catchup=False,
    default_args=default_args
):

    run_all = PythonOperator(
        task_id="run_all_tasks",
        python_callable=run_all_tasks
    )

    # trigger_load_redshift = TriggerDagRunOperator(
    #     task_id="trigger_load_s3_to_redshift",
    #     trigger_dag_id="load_s3_to_redshift",
    #     wait_for_completion=False,
    #     reset_dag_run=False
    # )

    # run_all ëë‚˜ë©´ extract_kakao_url DAG ì‹¤í–‰ë¨
    # run_all >> trigger_load_redshift
    run_all
