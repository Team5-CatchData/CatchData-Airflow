import pandas as pd
import requests
from datetime import datetime, timedelta

from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.common.sql.operators.sql import SQLExecuteQueryOperator
from airflow.providers.postgres.hooks.postgres import PostgresHook
from sqlalchemy import Numeric, String, Integer, BigInteger, Float, text

# =========================
# ê¸°ë³¸ ì„¤ì •
# =========================
REDSHIFT_CONN_ID = "redshift_conn"
SCHEMA_NAME = "analytics"
RAW_TABLE1 = "raw_data.kakao_crawl"
RAW_TABLE2 = "analytics.realtime_waiting"
RAW_TABLE3 = "analytics.derived_features_base"
FINAL_TABLE_NAME = "map_search"
SLACK_WEBHOOK_URL = ("https://hooks.slack.com/services/T09SZ0BSHEU"
                     "/B0A3W3R4H9D/Ea5DqrFBnQKc3SzbSuNhcmZo")

# =========================
# ðŸ’¡ SQL: ìµœì¢… í…Œì´ë¸” ìƒì„± ìŠ¤í‚¤ë§ˆ
# =========================
FINAL_TABLE_CREATE_SQL = f"""
CREATE TABLE IF NOT EXISTS {SCHEMA_NAME}.{FINAL_TABLE_NAME} (
    id BIGINT,
    name VARCHAR(50),
    region VARCHAR(50),
    city VARCHAR(50),
    category VARCHAR(100),
    x FLOAT,
    y FLOAT,
    waiting INTEGER,
    rating FLOAT,
    phone VARCHAR(50),
    image_url VARCHAR(500),
    address VARCHAR(300),
    rec_quality FLOAT,
    rec_balanced FLOAT,
    rec_convenience FLOAT,
    cluster INTEGER
) DISTSTYLE EVEN;
"""

def full_static_feature_pipeline():
    """
    ì›ë³¸ ë° ì‹¤ì‹œê°„ í…Œì´ë¸”ì„ ì¡°ì¸í•˜ì—¬ ê²€ìƒ‰ ì „ìš© í…Œì´ë¸”ì„ ìƒì„±í•˜ê³  ì›ìžì ìœ¼ë¡œ ê°±ì‹ í•©ë‹ˆë‹¤.
    """

    # Redshift Hook ì´ˆê¸°í™”
    redshift_hook = PostgresHook(postgres_conn_id=REDSHIFT_CONN_ID)
    engine = redshift_hook.get_sqlalchemy_engine()

    # 1. Redshiftì—ì„œ í†µí•© ë°ì´í„° ë¡œë“œ
    print("--- 1. Redshiftì—ì„œ ì›ë³¸ ë°ì´í„° ë¡œë“œ ì‹œìž‘ ---")
    sql_select = f"""
    SELECT 
        A.id,
        A.place_name as name,
        A.address_name as address,
        A.category_name,
        A.x,
        A.y,
        B.waiting,
        A.rating,
        A.phone,
        A.img_url as image_url,
        B.rec_quality,
        B.rec_balanced,
        B.rec_convenience,
        C.cluster
    FROM {RAW_TABLE1} A
    INNER JOIN {RAW_TABLE2} B 
        ON A.id = B.id
    LEFT JOIN {RAW_TABLE3} C
        ON A.id = C.id;
    """

    df = redshift_hook.get_pandas_df(sql_select)

    if df.empty:
        print("ê²½ê³ : ì›ë³¸ í…Œì´ë¸”ì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. íŒŒì´í”„ë¼ì¸ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return

    # [ì¤‘ë³µ ì œê±° ë¡œì§ ì¶”ê°€] id ê¸°ì¤€ìœ¼ë¡œ í•˜ë‚˜ë§Œ ë‚¨ê¹€
    # print(f"ì¤‘ë³µ ì œê±° ì „ ë°ì´í„°: {len(df)}ê°œ")
    # df = df.drop_duplicates(subset=['id'], keep='first')
    # print(f"âœ… ì¤‘ë³µ ì œê±° í›„ ë°ì´í„°: {len(df)}ê°œ")

    # 2. ë°ì´í„° ì „ì²˜ë¦¬
    print("--- 2. ë°ì´í„° ì „ì²˜ë¦¬ ì‹œìž‘ ---")
    df['category'] = df['category_name'].str.split('>').str[-1].str.strip()
    address_split = df['address'].str.split(' ', n=2, expand=True)
    df['region'] = address_split[0]
    df['city'] = address_split[1]

    final_df = df[['id', 'name', 'region', 'city', 'category', 'x', 'y', 
                   'waiting', 'rating', 'phone', 'image_url', 'address',
                   'rec_quality', 'rec_balanced', 'rec_convenience', 'cluster']].copy()

    # 3. Redshift ì›ìžì  êµì²´ ì‹¤í–‰
    STAGING_TABLE = f"{FINAL_TABLE_NAME}_staging"
    BACKUP_TABLE = f"{FINAL_TABLE_NAME}_old"

    # 3-1. [ì—ëŸ¬ ë°©ì§€] Staging í…Œì´ë¸” ëª…ì‹œì  ì‚­ì œ
    print(f"--- 3-1. ê¸°ì¡´ Staging í…Œì´ë¸” ì •ë¦¬: {STAGING_TABLE} ---")
    with engine.connect() as conn:
        conn.execute(text(f"DROP TABLE IF EXISTS {SCHEMA_NAME}.{STAGING_TABLE};"))
        conn.execute(text("COMMIT;"))

    # ë°ì´í„° íƒ€ìž… ë§¤í•‘
    dtype_mapping = {
        'id': BigInteger(),
        'x': Numeric(15, 12),
        'y': Numeric(15, 12),
        'rating': Numeric(3, 1),
        'name': String(50),
        'category': String(50),
        'image_url': String(500),
        'address': String(300),
        'rec_quality': Numeric(15, 14),
        'rec_balanced': Numeric(15, 14),
        'rec_convenience': Numeric(15, 14)
    }

    # Staging í…Œì´ë¸” ë¡œë“œ (if_exists='fail'ë¡œ ì„¤ì •í•˜ì—¬ ì¶©ëŒ ë°©ì§€)
    print(f"--- 3-2. Staging í…Œì´ë¸” ë¡œë“œ ì‹¤í–‰ ---")
    final_df.to_sql(
        name=STAGING_TABLE,
        con=engine,
        schema=SCHEMA_NAME,
        if_exists='fail', 
        index=False,
        dtype=dtype_mapping
    )

    # 3-3. íŠ¸ëžœìž­ì…˜ ì‹œìž‘ (Atomic Swap)
    # RENAME êµ¬ë¬¸ì—ì„œ ìŠ¤í‚¤ë§ˆëª…ì„ ì œì™¸í•˜ì—¬ Redshift í‘œì¤€ ì¤€ìˆ˜
    sql_commands = f"""
    BEGIN;
    DROP TABLE IF EXISTS {SCHEMA_NAME}.{BACKUP_TABLE};
    ALTER TABLE {SCHEMA_NAME}.{FINAL_TABLE_NAME} RENAME TO {BACKUP_TABLE};
    ALTER TABLE {SCHEMA_NAME}.{STAGING_TABLE} RENAME TO {FINAL_TABLE_NAME};
    COMMIT;
    DROP TABLE IF EXISTS {SCHEMA_NAME}.{BACKUP_TABLE};
    """

    redshift_hook.run(sql_commands)
    print(f"âœ… {SCHEMA_NAME}.{FINAL_TABLE_NAME} ê°±ì‹  ì™„ë£Œ")
    
    # Slack ì•Œë¦¼
    payload = {"text": (
        f"*ver2_05_map_search.py*\n"
        f"ðŸ“Œ âœ… {SCHEMA_NAME}.{FINAL_TABLE_NAME} ê°±ì‹  ì™„ë£Œ {len(final_df)} rows\n")}
    requests.post(SLACK_WEBHOOK_URL, json=payload, timeout=10)

# =========================
# DAG ì •ì˜
# =========================
default_args = {
    "owner": "airflow",
    "start_date": datetime(2025, 1, 1),
    "retries": 1,
    "retry_delay": timedelta(minutes=5)
}

with DAG(
    dag_id="ver2_05_map_search",
    default_args=default_args,
    description="ì¹´ì¹´ì˜¤ ì›ë³¸ ë°ì´í„°ì™€ ì‹¤ì‹œê°„ ëŒ€ê¸° ë°ì´í„°ë¥¼ í•©ì³ ì§€ë„ ê²€ìƒ‰ìš© í…Œì´ë¸” ìƒì„±",
    schedule=None,
    catchup=False,
    max_active_runs=1  # ðŸš¨ ë™ì‹œ ì‹¤í–‰ìœ¼ë¡œ ì¸í•œ íŠ¸ëžœìž­ì…˜ ì¶©ëŒ ë°©ì§€
) as dag:

    t0_create_table = SQLExecuteQueryOperator(
        task_id="create_final_table_if_not_exists",
        conn_id=REDSHIFT_CONN_ID,
        sql=FINAL_TABLE_CREATE_SQL,
    )

    t1_full_pipeline = PythonOperator(
        task_id="run_full_static_feature_pipeline",
        python_callable=full_static_feature_pipeline,
    )

    t0_create_table >> t1_full_pipeline