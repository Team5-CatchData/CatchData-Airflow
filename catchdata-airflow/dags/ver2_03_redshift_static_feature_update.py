import io
import json
import math
from datetime import datetime, timedelta

import boto3
import joblib
import pandas as pd
import requests
from airflow.providers.common.sql.operators.sql import (
    SQLExecuteQueryOperator,  # í…Œì´ë¸” ìƒì„±ìš©
)
from airflow.providers.postgres.hooks.postgres import PostgresHook
from airflow.providers.standard.operators.python import PythonOperator
from airflow.sdk import DAG
from sqlalchemy import Numeric, String

# =========================
# ê¸°ë³¸ ì„¤ì •
# =========================
BUCKET_NAME = "team5-batch"

REDSHIFT_CONN_ID = "redshift_conn"
SCHEMA_NAME = "analytics"
RAW_TABLE = "raw_data.kakao_crawl"
FINAL_TABLE_NAME = "derived_features_base"

SLACK_WEBHOOK_URL = ("https://hooks.slack.com/services/T09SZ0BSHEU"
                     "/B0A3W3R4H9D/Ea5DqrFBnQKc3SzbSuNhcmZo")

# ê°€ì¤‘ì¹˜ ì„¤ì • (base_population ê³„ì‚°ìš©)
W_REVIEW = 1.0
W_BLOG = 0.7

# 24ê°œ ì‹œê°„ëŒ€ ì»¬ëŸ¼ ì´ë¦„ ì •ì˜
TIME_COLUMNS = [f'time{i}' for i in range(24)]

# =========================
# ðŸ’¡ SQL: ìµœì¢… í…Œì´ë¸” ìƒì„± ìŠ¤í‚¤ë§ˆ
# =========================
# Redshiftì— ìµœì í™”ëœ í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ ì •ì˜
FINAL_TABLE_CREATE_SQL = f"""
CREATE TABLE IF NOT EXISTS {SCHEMA_NAME}.{FINAL_TABLE_NAME} (
    id BIGINT PRIMARY KEY,
    category VARCHAR(100),
    base_population NUMERIC(18, 4),
    quality_score NUMERIC(18, 4),
    rating NUMERIC(3, 2),
    -- 24ê°œ ì‹œê°„ëŒ€ ì»¬ëŸ¼ (ë°©ë¬¸ìž ìˆ˜ëŠ” ìž‘ìœ¼ë¯€ë¡œ SMALLINT ì‚¬ìš©)
    {', '.join([f'{col} SMALLINT' for col in TIME_COLUMNS])},
    cluster SMALLINT,
    calculated_at TIMESTAMP
)
-- idë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë°ì´í„° ë¶„ì‚° ë° ì •ë ¬í•˜ì—¬ ì¡°ì¸ ë° ì¿¼ë¦¬ ì„±ëŠ¥ ìµœì í™”
DISTKEY(id)
SORTKEY(calculated_at);
"""

# =========================
# ðŸ’¡ ë³´ì¡° í•¨ìˆ˜: S3ì—ì„œ ê°ì²´ ë¡œë“œ
# =========================
def load_from_s3(bucket, key):
    # Airflow Connectionsì— ì„¤ì •ëœ AWS ìžê²©ì¦ëª…ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¢‹ìœ¼ë‚˜,
    # ì—¬ê¸°ì„œëŠ” ì§ì ‘ ìž…ë ¥ì„ ê°€ì •í•œ ê¸°ë³¸ êµ¬ì¡°ë¡œ ìž‘ì„±í•©ë‹ˆë‹¤.
    s3 = boto3.client(
        "s3"
        )
    try:
        response = s3.get_object(Bucket=bucket, Key=key)
        return joblib.load(io.BytesIO(response['Body'].read()))
    except s3.exceptions.NoSuchKey:
        # ì—ëŸ¬ ë°œìƒ ì‹œ ë²„í‚· ë‚´ íŒŒì¼ ëª©ë¡ì„ ì¶œë ¥í•˜ì—¬ ê²½ë¡œë¥¼ í™•ì¸í•˜ë„ë¡ ìœ ë„
        print(f"âŒ ì—ëŸ¬: {bucket} ë²„í‚·ì— {key} íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        objs = s3.list_objects_v2(Bucket=bucket, Prefix='models/') # models í´ë” ëª©ë¡ ì¡°íšŒ
        print("í˜„ìž¬ ì¡´ìž¬í•˜ëŠ” íŒŒì¼ë“¤:")
        for obj in objs.get('Contents', []):
            print(f" - {obj['Key']}")
        raise # ì—ëŸ¬ë¥¼ ë‹¤ì‹œ ë˜ì ¸ì„œ Airflowì—ì„œ í™•ì¸ ê°€ëŠ¥í•˜ê²Œ í•¨



# =========================
# ðŸ’¡ ë‹¨ì¼ í†µí•© í•¨ìˆ˜: ëª¨ë“  ë¡œì§ì„ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰ (Atomic Replacement)
# =========================
def full_static_feature_pipeline():
    """
    hourly_visit JSONì„ 24ê°œ ì»¬ëŸ¼ìœ¼ë¡œ ë³€í™˜í•˜ê³ ,
    Redshift í…Œì´ë¸” ì´ë¦„ êµì²´ë¥¼ í†µí•´ ì›ìžì ìœ¼ë¡œ ê°±ì‹ í•©ë‹ˆë‹¤.
    """

    # Redshift Hook ì´ˆê¸°í™”
    redshift_hook = PostgresHook(postgres_conn_id=REDSHIFT_CONN_ID)
    engine = redshift_hook.get_sqlalchemy_engine()

    # 1. Redshiftì—ì„œ ì›ë³¸ ë°ì´í„° ë¡œë“œ
    print("--- 1. Redshiftì—ì„œ ì›ë³¸ ë°ì´í„° ë¡œë“œ ì‹œìž‘ ---")
    sql_select = f"""
    SELECT
        id,
        category_name,
        rating,
        review_count,
        blog_count,
        hourly_visit
    FROM {RAW_TABLE};
    """

    df = redshift_hook.get_pandas_df(sql_select)

    if df.empty:
        print(f"ê²½ê³ : {RAW_TABLE} í…Œì´ë¸”ì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. íŒŒì´í”„ë¼ì¸ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return

    print(f"âœ… ì›ë³¸ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(df)}ê°œ")


    # 2. íŒŒìƒ ë³€ìˆ˜ ê³„ì‚° ë° hourly_visit ë¶„ë¦¬ (Python/Pandas í™˜ê²½)
    print("--- 2. íŒŒìƒ ë³€ìˆ˜ ê³„ì‚° ë° hourly_visit ë¶„ë¦¬ ì‹œìž‘ ---")

    # --- base_population ê³„ì‚° ---
    df['base_population'] = (
        df['review_count'].apply(math.log1p) * W_REVIEW +
        df['blog_count'].apply(math.log1p) * W_BLOG
    )

    # --- quality_score ê³„ì‚° ---
    df['quality_score'] = df['base_population'] * df['rating'].astype(float, errors='ignore')


    # --- hourly_visit JSON íŒŒì‹± ë° 24ê°œ ì»¬ëŸ¼ ë¶„ë¦¬ ---
    def safe_loads(json_str):
        """JSON íŒŒì‹± ì¤‘ ì˜¤ë¥˜ ë°œìƒ ì‹œ 0ìœ¼ë¡œ ì±„ì›Œì§„ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜"""
        try:
            if pd.isna(json_str) or json_str is None:
                return [0] * 24
            return json.loads(json_str)
        except Exception:
            # ë¦¬ìŠ¤íŠ¸ ê¸¸ì´ê°€ 24ê°€ ì•„ë‹Œ ê²½ìš°ì—ë„ 0ìœ¼ë¡œ ì±„ì›Œì§„ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
            return [0] * 24

    df['hourly_list'] = df['hourly_visit'].apply(safe_loads)

    # 24ê°œ ì‹œê°„ëŒ€ë³„ ì»¬ëŸ¼ ìƒì„±
    # Redshiftì˜ ìž‘ì€ ì •ìˆ˜í˜•(SMALLINT)ìœ¼ë¡œ ì €ìž¥í•˜ê¸° ìœ„í•´ íƒ€ìž… ë³€í™˜
    df[TIME_COLUMNS] = pd.DataFrame(df['hourly_list'].to_list(), index=df.index).astype('int16')
    df.drop(columns=['hourly_list', 'hourly_visit'], inplace=True)

    # í´ëŸ¬ìŠ¤í„°ë§
    ## --- category ì¶”ì¶œ ---
    df['category'] = df['category_name'].str.split('>').str[1].str.strip()

    ## --- ì‹œê°„ëŒ€ë³„ ë°©ë¬¸ ì¸ì› ë‚˜ëˆ„ê¸° ---
    df['breakfast'] = df[['time6','time7','time8','time9','time10']].sum(axis=1)
    df['lunch'] = df[['time11','time12','time13','time14','time15']].sum(axis=1)
    df['dinner'] = df[['time17','time18','time19','time20','time21']].sum(axis=1)
    df['late_night'] = df[['time21','time22','time23','time0','time1']].sum(axis=1)
    df['over_night'] = df[['time2','time3','time4','time5']].sum(axis=1)

    ## ì›-í•« ì¸ì½”ë”©
    clustering_df = df[['id', 'category', 'base_population', 'quality_score',
                        'breakfast', 'lunch', 'dinner', 'late_night', 'over_night']]
    df_dummy = pd.get_dummies(clustering_df, columns=['category'], dtype=int)

    # 4. ëª¨ë¸ ë¡œë“œ ë° ì˜ˆì¸¡
    print("--- ML ëª¨ë¸ ë¡œë“œ ë° ì˜ˆì¸¡ ì‹œìž‘ ---")
    model = load_from_s3(BUCKET_NAME, "models/kmeans_model_v1.pkl")
    scaler = load_from_s3(BUCKET_NAME, "models/scaler_v1.pkl")

    # [ì¤‘ìš”] í•™ìŠµ ì‹œ ì‚¬ìš©í–ˆë˜ ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸ ë¡œë“œ (ì»¬ëŸ¼ ìˆœì„œ/ê°œìˆ˜ ì¼ì¹˜ í•„ìˆ˜)
    # ëª¨ë¸ ì €ìž¥ ì‹œ í•¨ê»˜ ì €ìž¥í–ˆë˜ ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸ë¥¼ ë¶ˆëŸ¬ì˜¨ë‹¤ê³  ê°€ì •
    train_cols = load_from_s3(BUCKET_NAME, "models/train_columns.pkl")

    # í˜„ìž¬ ë°ì´í„°ì— ì—†ëŠ” ì¹´í…Œê³ ë¦¬ ì»¬ëŸ¼ì€ 0ìœ¼ë¡œ ì±„ìš°ê³ , í•™ìŠµ ì‹œ ì—†ë˜ ì»¬ëŸ¼ì€ ì œê±°
    for col in train_cols:
        if col not in df_dummy.columns:
            df_dummy[col] = 0

    # í•™ìŠµ ì‹œì™€ ë™ì¼í•œ ì»¬ëŸ¼ ìˆœì„œë¡œ ì •ë ¬ (id ì œì™¸)
    X = df_dummy[train_cols].drop(columns=['id'], errors='ignore')

    # ìŠ¤ì¼€ì¼ë§ ë° ì˜ˆì¸¡
    X_scaled = scaler.transform(X)
    df['cluster'] = model.predict(X_scaled)

    # 5. ìµœì¢… ë°ì´í„° ì •ë¦¬
    final_df = df[['id', 'category', 'base_population', 'quality_score', 'rating',
                   *TIME_COLUMNS, 'cluster']].copy()
    final_df['calculated_at'] = datetime.now()


    print("âœ… íŒŒìƒ ë³€ìˆ˜ ë° ì‹œê°„ëŒ€ ì»¬ëŸ¼ ê³„ì‚° ì™„ë£Œ")


    # 3. Redshift í…Œì´ë¸” ì´ë¦„ ë³€ê²½ì„ í†µí•œ ì›ìžì  êµì²´
    print("--- 3. Redshift í…Œì´ë¸” ì´ë¦„ êµì²´ ì‹œìž‘ (Atomic Replacement) ---")

    # ðŸ’¡ ìž„ì‹œ í…Œì´ë¸” ë° ë°±ì—… í…Œì´ë¸” ì´ë¦„ ì •ì˜
    STAGING_TABLE = 'derived_features_staging'
    BACKUP_TABLE = 'derived_features_old'

    # ðŸ’¡ ë°ì´í„° íƒ€ìž… ë§¤í•‘ ì •ì˜ (Redshift SMALLINTë¡œ ë§¤í•‘í•˜ê¸° ìœ„í•´ ëª…ì‹œ)
    dtype_mapping = {
        'category': String(50),
        'base_population': Numeric(18, 4),
        'quality_score': Numeric(18, 4),
        'rating': Numeric(3, 2),
        'cluster': Numeric(3, 0)
        # TIME_COLUMNSì˜ íƒ€ìž…ì€ int16ì„ í†µí•´ SMALLINTë¡œ ìžë™ìœ¼ë¡œ ì¶”ë¡ ë˜ë„ë¡ í•©ë‹ˆë‹¤.
    }

    # 3-1. ê³„ì‚°ëœ final_dfë¥¼ ìž„ì‹œ Staging í…Œì´ë¸”ì— ë¡œë“œ
    final_df.to_sql(
        name=STAGING_TABLE,
        con=engine,
        schema=SCHEMA_NAME,
        if_exists='replace',
        index=False,
        dtype=dtype_mapping
    )

    print(f"   -> Staging í…Œì´ë¸” ë¡œë“œ ì™„ë£Œ: {SCHEMA_NAME}.{STAGING_TABLE}")

    # 3-2. Redshift íŠ¸ëžœìž­ì…˜ ì‹œìž‘ ë° í…Œì´ë¸” ì´ë¦„ êµì²´ ì‹¤í–‰
    sql_commands = f"""
    BEGIN;

    -- 1. ê¸°ì¡´ ìµœì¢… í…Œì´ë¸”ì„ ë°±ì—… í…Œì´ë¸”ë¡œ ì´ë¦„ ë³€ê²½
    ALTER TABLE {SCHEMA_NAME}.{FINAL_TABLE_NAME} RENAME TO {BACKUP_TABLE};

    -- 2. ìž„ì‹œ í…Œì´ë¸”ì„ ìµœì¢… í…Œì´ë¸” ì´ë¦„ìœ¼ë¡œ ë³€ê²½ (ì›ìžì  êµì²´)
    ALTER TABLE {SCHEMA_NAME}.{STAGING_TABLE} RENAME TO {FINAL_TABLE_NAME};

    COMMIT;

    -- 3. ì´ì „ ë²„ì „ì˜ ë°±ì—… í…Œì´ë¸” ì •ë¦¬
    DROP TABLE IF EXISTS {SCHEMA_NAME}.{BACKUP_TABLE};
    """


    redshift_hook.run(sql_commands)

    print(f"âœ… {SCHEMA_NAME}.{FINAL_TABLE_NAME} í…Œì´ë¸”ì´ {len(final_df)}ê°œ ë ˆì½”ë“œë¡œ ì„œë¹„ìŠ¤ ì¤‘ë‹¨ ì—†ì´ ê°±ì‹ ë˜ì—ˆìŠµë‹ˆë‹¤.")
    payload = {"text": ("*ver2_03_redshift_static_feature_update.py*\n"
                        f"ðŸ“Œ âœ… {SCHEMA_NAME}.{FINAL_TABLE_NAME} í…Œì´ë¸”ì´ {len(final_df)}ê°œ ë ˆì½”ë“œë¡œ ì„œë¹„ìŠ¤ ì¤‘ë‹¨ ì—†ì´ ê°±ì‹ ë˜ì—ˆìŠµë‹ˆë‹¤.\n")}

    requests.post(
        SLACK_WEBHOOK_URL,
        json=payload,
        timeout=10,
    )

# =========================
# DAG ì •ì˜
# =========================
default_args = {
    "owner": "airflow",
    "start_date": datetime(2025, 1, 1),
    "retries": 1,
    "retry_delay": timedelta(minutes=5)
}

with DAG(
    dag_id="ver2_03_redshift_static_feature_update",
    default_args=default_args,
    description="hourly_visitì„ 24ê°œ time ì»¬ëŸ¼ìœ¼ë¡œ ë¶„ë¦¬í•˜ê³  RENAMEì„ í†µí•´ Redshift í…Œì´ë¸”ì„ ì›ìžì ìœ¼ë¡œ ê°±ì‹ í•©ë‹ˆë‹¤.",
    schedule=None,
    catchup=False
) as dag:

    # T0. ìµœì¢… í…Œì´ë¸”ì´ ì—†ëŠ” ê²½ìš° ìƒì„± (ìµœì´ˆ ì‹¤í–‰ ì‹œ ì•ˆì •ì„± í™•ë³´)
    t0_create_table = SQLExecuteQueryOperator(
        task_id="create_final_table_if_not_exists",
        conn_id=REDSHIFT_CONN_ID,
        sql=FINAL_TABLE_CREATE_SQL,
    )

    # T1. ë°ì´í„° ë¡œë“œ, ê³„ì‚° ë° ìµœì¢… í…Œì´ë¸” ê°±ì‹ 
    t1_full_pipeline = PythonOperator(
        task_id="run_full_static_feature_pipeline",
        python_callable=full_static_feature_pipeline,
    )

    # íŒŒì´í”„ë¼ì¸ íë¦„ ì •ì˜: í…Œì´ë¸” ìƒì„± í™•ì¸ í›„ ë°ì´í„° ê°±ì‹ 
    t0_create_table >> t1_full_pipeline
